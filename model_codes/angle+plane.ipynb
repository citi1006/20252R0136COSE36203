{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Install Mediapipe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ipKyFljfvoTk",
        "outputId": "a59ba287-ed41-435a-d0a3-dc4752c946a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting mediapipe\n",
            "  Downloading mediapipe-0.10.21-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (25.4.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (25.9.23)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.7.2)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.7.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from mediapipe) (3.10.0)\n",
            "Collecting numpy<2 (from mediapipe)\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.12/dist-packages (from mediapipe) (4.12.0.88)\n",
            "Collecting protobuf<5,>=4.25.3 (from mediapipe)\n",
            "  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Collecting sounddevice>=0.4.4 (from mediapipe)\n",
            "  Downloading sounddevice-0.5.3-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.2.1)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.12/dist-packages (from sounddevice>=0.4.4->mediapipe) (2.0.0)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe) (0.5.4)\n",
            "INFO: pip is looking at multiple versions of jax to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting jax (from mediapipe)\n",
            "  Downloading jax-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting jaxlib (from mediapipe)\n",
            "  Downloading jaxlib-0.8.1-cp312-cp312-manylinux_2_27_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting jax (from mediapipe)\n",
            "  Downloading jax-0.8.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting jaxlib (from mediapipe)\n",
            "  Downloading jaxlib-0.8.0-cp312-cp312-manylinux_2_27_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting jax (from mediapipe)\n",
            "  Downloading jax-0.7.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting jaxlib (from mediapipe)\n",
            "  Downloading jaxlib-0.7.1-cp312-cp312-manylinux_2_27_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.12 in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe) (1.16.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
            "INFO: pip is looking at multiple versions of opencv-contrib-python to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting opencv-contrib-python (from mediapipe)\n",
            "  Downloading opencv_contrib_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.23)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n",
            "Downloading mediapipe-0.10.21-cp312-cp312-manylinux_2_28_x86_64.whl (35.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.6/35.6 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sounddevice-0.5.3-py3-none-any.whl (32 kB)\n",
            "Downloading jax-0.7.1-py3-none-any.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jaxlib-0.7.1-cp312-cp312-manylinux_2_27_x86_64.whl (81.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.2/81.2 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencv_contrib_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (69.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: protobuf, numpy, sounddevice, opencv-contrib-python, jaxlib, jax, mediapipe\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.5\n",
            "    Uninstalling protobuf-5.29.5:\n",
            "      Successfully uninstalled protobuf-5.29.5\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: opencv-contrib-python\n",
            "    Found existing installation: opencv-contrib-python 4.12.0.88\n",
            "    Uninstalling opencv-contrib-python-4.12.0.88:\n",
            "      Successfully uninstalled opencv-contrib-python-4.12.0.88\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.7.2\n",
            "    Uninstalling jaxlib-0.7.2:\n",
            "      Successfully uninstalled jaxlib-0.7.2\n",
            "  Attempting uninstall: jax\n",
            "    Found existing installation: jax 0.7.2\n",
            "    Uninstalling jax-0.7.2:\n",
            "      Successfully uninstalled jax-0.7.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "opentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 4.25.8 which is incompatible.\n",
            "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed jax-0.7.1 jaxlib-0.7.1 mediapipe-0.10.21 numpy-1.26.4 opencv-contrib-python-4.11.0.86 protobuf-4.25.8 sounddevice-0.5.3\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "49e83b1964dd47b3b48b90e62a13db55",
              "pip_warning": {
                "packages": [
                  "google",
                  "numpy"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install mediapipe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Helper Func"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_angle(a, b, c):\n",
        "    \"\"\"\n",
        "    세 개의 랜드마크(a, b, c)를 받아 b 지점의 각도와 신뢰도(가시성 곱)를 반환합니다.\n",
        "    \"\"\"\n",
        "    reliability = a[3] * b[3] * c[3]\n",
        "\n",
        "    if a[3] < 0.5 or b[3] < 0.5 or c[3] < 0.5:\n",
        "        return None, reliability\n",
        "\n",
        "    a_xyz = a[:3]\n",
        "    b_xyz = b[:3]\n",
        "    c_xyz = c[:3]\n",
        "\n",
        "    try:\n",
        "        ba = a_xyz - b_xyz\n",
        "        bc = c_xyz - b_xyz\n",
        "\n",
        "        cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))\n",
        "        cosine_angle = np.clip(cosine_angle, -1.0, 1.0)\n",
        "        angle = np.arccos(cosine_angle)\n",
        "\n",
        "        return np.degrees(angle), reliability\n",
        "    except (ZeroDivisionError, RuntimeWarning):\n",
        "        return None, reliability\n",
        "\n",
        "def calculate_plane_normal(a, b, c):\n",
        "    \"\"\"\n",
        "    세 개의 랜드마크(a, b, c)로 정의된 평면의 법선 벡터(Normal Vector)를 반환합니다.\n",
        "    반환값: [x, y, z] (정규화된 벡터), reliability\n",
        "    순서는 a -> b -> c 순으로 오른손 법칙을 따릅니다.\n",
        "    \"\"\"\n",
        "    # 신뢰도 계산\n",
        "    reliability = a[3] * b[3] * c[3]\n",
        "\n",
        "    # 가시성이 낮은 랜드마크가 포함되면 계산 불가\n",
        "    if a[3] < 0.5 or b[3] < 0.5 or c[3] < 0.5:\n",
        "        return None, reliability\n",
        "\n",
        "    a_xyz = a[:3]\n",
        "    b_xyz = b[:3]\n",
        "    c_xyz = c[:3]\n",
        "\n",
        "    try:\n",
        "        # 두 개의 벡터 생성 (a -> b, a -> c)\n",
        "        v1 = b_xyz - a_xyz\n",
        "        v2 = c_xyz - a_xyz\n",
        "\n",
        "        # 외적(Cross Product)을 통해 법선 벡터 계산\n",
        "        normal = np.cross(v1, v2)\n",
        "\n",
        "        # 벡터 정규화 (크기를 1로 만듦)\n",
        "        norm_length = np.linalg.norm(normal)\n",
        "        if norm_length == 0:\n",
        "            return None, reliability\n",
        "\n",
        "        normalized_normal = normal / norm_length\n",
        "\n",
        "        return normalized_normal, reliability\n",
        "\n",
        "    except (RuntimeWarning, Exception):\n",
        "        return None, reliability\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "def feature_engineering(landmarks):\n",
        "    \"\"\"\n",
        "    (33, 4) 랜드마크 배열을 입력받아 특징 벡터를 생성합니다.\n",
        "    특징 구성:\n",
        "      1. 각도 (Angles): 6개 (정규화됨)\n",
        "      2. 법선 벡터 (Normals): 3개 * 3축(x,y,z) = 9개\n",
        "      3. 각도 신뢰도: 6개\n",
        "      4. 법선 신뢰도: 3개\n",
        "    총 차원: 6 + 9 + 6 + 3 = 24차원 (예상)\n",
        "    \"\"\"\n",
        "    if landmarks is None:\n",
        "        return None\n",
        "\n",
        "    # --- MediaPipe 랜드마크 인덱스 정의 ---\n",
        "    NOSE = 0\n",
        "    L_EYE = 2  # 왼쪽 눈동자\n",
        "    R_EYE = 5  # 오른쪽 눈동자\n",
        "    L_EAR, R_EAR = 7, 8\n",
        "    L_SHOULDER, R_SHOULDER = 11, 12\n",
        "    L_ELBOW, R_ELBOW = 13, 14\n",
        "    L_WRIST, R_WRIST = 15, 16\n",
        "    L_HIP, R_HIP = 23, 24\n",
        "\n",
        "    angle_features = []\n",
        "    angle_reliabilities = []\n",
        "\n",
        "    normal_features = []   # 법선 벡터 (x, y, z) 저장\n",
        "    normal_reliabilities = [] # 법선 벡터 신뢰도 저장\n",
        "\n",
        "    # ==========================================\n",
        "    # [Part 1] 각도 특징 (기존 로직 유지)\n",
        "    # ==========================================\n",
        "\n",
        "    # 1. 팔꿈치\n",
        "    angles_to_calc = [\n",
        "        (landmarks[L_SHOULDER], landmarks[L_ELBOW], landmarks[L_WRIST]),\n",
        "        (landmarks[R_SHOULDER], landmarks[R_ELBOW], landmarks[R_WRIST]),\n",
        "        (landmarks[L_ELBOW], landmarks[L_SHOULDER], landmarks[L_HIP]),\n",
        "        (landmarks[R_ELBOW], landmarks[R_SHOULDER], landmarks[R_HIP]),\n",
        "        (landmarks[L_EAR], landmarks[L_SHOULDER], landmarks[L_HIP]),\n",
        "        (landmarks[R_EAR], landmarks[R_SHOULDER], landmarks[R_HIP])\n",
        "    ]\n",
        "\n",
        "    for p1, p2, p3 in angles_to_calc:\n",
        "        ang, rel = calculate_angle(p1, p2, p3)\n",
        "        angle_features.append(ang)\n",
        "        angle_reliabilities.append(rel)\n",
        "\n",
        "    # ==========================================\n",
        "    # [Part 2] 법선 벡터(Normal) 특징 추가 (NEW)\n",
        "    # ==========================================\n",
        "\n",
        "    # 2. 얼굴(눈+코) Normal\n",
        "    # 정의: Nose, L_Eye, R_Eye. 순서에 따라 얼굴 정면 방향이 됨\n",
        "    face_normal, face_rel = calculate_plane_normal(landmarks[NOSE], landmarks[L_EYE], landmarks[R_EYE])\n",
        "    normal_features.append(face_normal)\n",
        "    normal_reliabilities.append(face_rel)\n",
        "\n",
        "    # 3. 어깨+코 Normal\n",
        "    # 정의: L_Shoulder, R_Shoulder, Nose\n",
        "    sh_nose_normal, sh_nose_rel = calculate_plane_normal(landmarks[L_SHOULDER], landmarks[R_SHOULDER], landmarks[NOSE])\n",
        "    normal_features.append(sh_nose_normal)\n",
        "    normal_reliabilities.append(sh_nose_rel)\n",
        "\n",
        "\n",
        "    # ==========================================\n",
        "    # [Part 3] 데이터 후처리 및 벡터 병합\n",
        "    # ==========================================\n",
        "\n",
        "    # 1. 각도 정규화 (None -> 0.0, 값 -> 0~1)\n",
        "    norm_angles_vec = [(val / 360.0) if val is not None else 0.0 for val in angle_features]\n",
        "\n",
        "    # 2. 법선 벡터 평탄화 (Flatten)\n",
        "    # normal_features는 [[x,y,z], None, [x,y,z]] 형태일 수 있음\n",
        "    flat_normals_vec = []\n",
        "    for vec in normal_features:\n",
        "        if vec is not None:\n",
        "            flat_normals_vec.extend(vec) # [x, y, z] 추가\n",
        "        else:\n",
        "            flat_normals_vec.extend([0.0, 0.0, 0.0]) # 실패 시 0 벡터 채움\n",
        "\n",
        "    # 3. 최종 결합\n",
        "    # [정규화 각도들] + [법선 벡터들(x,y,z, x,y,z...)] + [각도 신뢰도들] + [법선 신뢰도들]\n",
        "    final_features = norm_angles_vec + flat_normals_vec + normal_reliabilities + angle_reliabilities\n",
        "    #final_features = norm_angles_vec + flat_normals_vec\n",
        "\n",
        "    return np.array(final_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MLP(val_accuracy = 0.9670)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NOVr50REq-KD",
        "outputId": "26ef86ca-1cf7-43f3-ccd5-eb983294827f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jaxlib/plugin_support.py:71: RuntimeWarning: JAX plugin jax_cuda12_plugin version 0.7.2 is installed, but it is not compatible with the installed jaxlib version 0.7.1, so it will not be used.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'X_train.npy'와 'Y_train.npy'에서 데이터셋 로드 중...\n",
            "로드 완료. 랜드마크: (2139, 33, 4), 레이블: (2139,)\n",
            "\n",
            "특징 공학(Feature Engineering) 적용 중...\n",
            "학습 데이터: 2139개 / 테스트 데이터: 250개\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,344</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m1,344\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,457</span> (13.50 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,457\u001b[0m (13.50 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,457</span> (13.50 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,457\u001b[0m (13.50 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "모델 학습 시작...\n",
            "Epoch 1/500\n",
            "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.6078 - loss: 0.6576 - val_accuracy: 0.5189 - val_loss: 0.6168\n",
            "Epoch 2/500\n",
            "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6512 - loss: 0.5927 - val_accuracy: 0.6085 - val_loss: 0.4518\n",
            "Epoch 3/500\n",
            "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7227 - loss: 0.5000 - val_accuracy: 0.8113 - val_loss: 0.3576\n",
            "Epoch 4/500\n",
            "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7886 - loss: 0.4273 - val_accuracy: 0.9198 - val_loss: 0.2923\n",
            "Epoch 5/500\n",
            "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8370 - loss: 0.3852 - val_accuracy: 0.8019 - val_loss: 0.3393\n",
            "Epoch 6/500\n",
            "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8795 - loss: 0.3261 - val_accuracy: 0.9434 - val_loss: 0.2241\n",
            "Epoch 7/500\n",
            "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9015 - loss: 0.2722 - val_accuracy: 0.6934 - val_loss: 0.4066\n",
            "Epoch 8/500\n",
            "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9069 - loss: 0.2540 - val_accuracy: 0.9198 - val_loss: 0.2319\n",
            "Epoch 9/500\n",
            "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9098 - loss: 0.2332 - val_accuracy: 0.9670 - val_loss: 0.1274\n",
            "Epoch 10/500\n",
            "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9231 - loss: 0.2215 - val_accuracy: 0.9104 - val_loss: 0.2444\n",
            "Epoch 11/500\n",
            "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9249 - loss: 0.2134 - val_accuracy: 0.9151 - val_loss: 0.2225\n",
            "Epoch 12/500\n",
            "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9313 - loss: 0.2028 - val_accuracy: 0.7453 - val_loss: 0.4320\n",
            "Epoch 13/500\n",
            "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9310 - loss: 0.1928 - val_accuracy: 0.9623 - val_loss: 0.1371\n",
            "Epoch 14/500\n",
            "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9358 - loss: 0.1795 - val_accuracy: 0.9670 - val_loss: 0.1137\n",
            "Epoch 15/500\n",
            "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9367 - loss: 0.1861 - val_accuracy: 0.9151 - val_loss: 0.2247\n",
            "Epoch 16/500\n",
            "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9346 - loss: 0.1773 - val_accuracy: 0.9670 - val_loss: 0.1348\n",
            "Epoch 17/500\n",
            "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9408 - loss: 0.1736 - val_accuracy: 0.9670 - val_loss: 0.1163\n",
            "Epoch 18/500\n",
            "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9410 - loss: 0.1674 - val_accuracy: 0.9623 - val_loss: 0.1432\n",
            "Epoch 19/500\n",
            "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9230 - loss: 0.1908 - val_accuracy: 0.9057 - val_loss: 0.2415\n",
            "Epoch 20/500\n",
            "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9372 - loss: 0.1831 - val_accuracy: 0.8868 - val_loss: 0.2621\n",
            "Epoch 21/500\n",
            "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9358 - loss: 0.1781 - val_accuracy: 0.9151 - val_loss: 0.2021\n",
            "Epoch 22/500\n",
            "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9388 - loss: 0.1542 - val_accuracy: 0.9670 - val_loss: 0.1504\n",
            "모델 학습 완료.\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7494 - loss: 0.5733 \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "테스트 데이터 평가 결과:\n",
            "Loss (손실): 1.1849\n",
            "Accuracy (정확도): 0.4720\n",
            "학습된 모델을 'pose_classifier_model.h5'로 저장했습니다.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Input\n",
        "\n",
        "def build_model(input_shape):\n",
        "    \"\"\"\n",
        "    특징 벡터를 입력받는 Keras 분류 모델을 구성합니다.\n",
        "    \"\"\"\n",
        "    model = Sequential([\n",
        "        Input(shape=(input_shape,)),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "        # 이진 분류를 위한 Sigmoid 활성화 함수\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=[tf.keras.metrics.BinaryAccuracy(name='accuracy')])\n",
        "    return model\n",
        "\n",
        "# --- 메인 실행 로직 ---\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # --- 1. 데이터 로드 ---\n",
        "    # .npy 파일 경로\n",
        "    X_DATA_PATH = 'X_train.npy'\n",
        "    Y_DATA_PATH = 'Y_train.npy'\n",
        "    X_VAL_PATH = 'X_val.npy'\n",
        "    Y_VAL_PATH = 'Y_val.npy'\n",
        "    X_TEST_PATH = 'X_test.npy'\n",
        "    Y_TEST_PATH = 'Y_test.npy'\n",
        "\n",
        "    print(f\"'{X_DATA_PATH}'와 '{Y_DATA_PATH}'에서 데이터셋 로드 중...\")\n",
        "\n",
        "    if not os.path.exists(X_DATA_PATH) or not os.path.exists(Y_DATA_PATH):\n",
        "        print(\"=\"*50)\n",
        "        print(f\"오류: '{X_DATA_PATH}' 또는 '{Y_DATA_PATH}' 파일을 찾을 수 없습니다.\")\n",
        "        print(\"데이터셋 파일이 스크립트와 동일한 디렉토리에 있는지 확인해주세요.\")\n",
        "        print(\"=\"*50)\n",
        "    else:\n",
        "        try:\n",
        "            # .npy 파일에서 각각 랜드마크와 레이블을 불러옵니다.\n",
        "            raw_landmarks = np.load(X_DATA_PATH)\n",
        "            raw_val = np.load(X_VAL_PATH)\n",
        "            raw_test = np.load(X_TEST_PATH)\n",
        "            labels = np.load(Y_DATA_PATH)\n",
        "            labels_val = np.load(Y_VAL_PATH)\n",
        "            labels_test = np.load(Y_TEST_PATH)\n",
        "            print(f\"로드 완료. 랜드마크: {raw_landmarks.shape}, 레이블: {labels.shape}\")\n",
        "\n",
        "            if raw_landmarks.shape[0] == 0:\n",
        "                print(\"오류: 로드된 데이터셋이 비어있습니다.\")\n",
        "            else:\n",
        "                # --- 2. 특징 공학 (Feature Engineering) ---\n",
        "\n",
        "                print(\"\\n특징 공학(Feature Engineering) 적용 중...\")\n",
        "                features = []\n",
        "                valid_labels = []\n",
        "\n",
        "                for i, lm in enumerate(raw_landmarks):\n",
        "                    feature_vector = feature_engineering(lm)\n",
        "                    # 특징 추출에 성공한(None이 아닌) 샘플만 사용\n",
        "                    if feature_vector is not None:\n",
        "                        features.append(feature_vector)\n",
        "                        valid_labels.append(labels[i])\n",
        "                features_val = []\n",
        "                valid_labels_val = []\n",
        "\n",
        "                for i, lm in enumerate(raw_val):\n",
        "                    feature_vector = feature_engineering(lm)\n",
        "                    # 특징 추출에 성공한(None이 아닌) 샘플만 사용\n",
        "                    if feature_vector is not None:\n",
        "                        features_val.append(feature_vector)\n",
        "                        valid_labels_val.append(labels_val[i])\n",
        "\n",
        "                features_test = []\n",
        "                valid_labels_test = []\n",
        "\n",
        "                for i, lm in enumerate(raw_test):\n",
        "                    feature_vector = feature_engineering(lm)\n",
        "                    # 특징 추출에 성공한(None이 아닌) 샘플만 사용\n",
        "                    if feature_vector is not None:\n",
        "                        features_test.append(feature_vector)\n",
        "                        valid_labels_test.append(labels_test[i])\n",
        "\n",
        "                # X: (n_valid_samples, n_features)\n",
        "                # y: (n_valid_samples,)\n",
        "                X_train = np.array(features)\n",
        "                y_train = np.array(valid_labels)\n",
        "                X_val = np.array(features_val)\n",
        "                y_val = np.array(valid_labels_val)\n",
        "                X_test = np.array(features_test)\n",
        "                y_test = np.array(valid_labels_test)\n",
        "\n",
        "                if X_train.shape[0] == 0:\n",
        "                     print(\"오류: 모든 샘플에서 특징 공학(각도 계산 등)에 실패했습니다.\")\n",
        "                     print(\"데이터셋의 가시성(visibility) 값을 확인해주세요.\")\n",
        "                else:\n",
        "\n",
        "                    print(f\"학습 데이터: {X_train.shape[0]}개 / 테스트 데이터: {X_test.shape[0]}개\")\n",
        "\n",
        "                    # --- 4. 모델 학습 ---\n",
        "                    # 1. 인덱스 배열 생성 (0부터 N-1까지)\n",
        "                    indices = np.arange(X_train.shape[0])\n",
        "\n",
        "                    # 2. 인덱스 무작위 섞기\n",
        "                    np.random.shuffle(indices)\n",
        "\n",
        "                    # 3. 섞인 인덱스로 X와 y 재배열 (짝 유지됨)\n",
        "                    X_train = X_train[indices]\n",
        "                    y_train = y_train[indices]\n",
        "\n",
        "                    # 입력 차원(input_shape)은 특징 벡터의 크기입니다. (여기서는 8)\n",
        "                    model = build_model(input_shape=X_train.shape[1])\n",
        "                    model.summary()\n",
        "\n",
        "                    print(\"\\n모델 학습 시작...\")\n",
        "                    history = model.fit(\n",
        "                        X_train,\n",
        "                        y_train,\n",
        "                        epochs=500,\n",
        "                        batch_size=16,\n",
        "                        validation_data=(X_val, y_val),\n",
        "                        callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True)]\n",
        "                    )\n",
        "\n",
        "                    print(\"모델 학습 완료.\")\n",
        "\n",
        "                    # --- 5. 모델 평가 ---\n",
        "                    loss, accuracy = model.evaluate(X_test, y_test)\n",
        "                    print(f\"\\n테스트 데이터 평가 결과:\")\n",
        "                    print(f\"Loss (손실): {loss:.4f}\")\n",
        "                    print(f\"Accuracy (정확도): {accuracy:.4f}\")\n",
        "\n",
        "                    # --- 6. 모델 저장 ---\n",
        "                    model.save(\"pose_classifier_keras.h5\")\n",
        "                    print(\"학습된 모델을 'pose_classifier_keras.h5'로 저장했습니다.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"오류: 데이터셋 로드 또는 처리 중 예외 발생: {e}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RandomForest (val_accuracy = 0.9575)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stotQbNEf7cP",
        "outputId": "75ca44c6-b5e9-42f7-c1d8-985e4efe07b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "데이터 전처리 중...\n",
            "데이터 셔플링 중...\n",
            "\n",
            "[Random Forest] 학습 시작 (데이터 개수: 2139개)...\n",
            "학습 완료.\n",
            "\n",
            "테스트 데이터 평가 결과:\n",
            "Accuracy (정확도): 0.9575\n",
            "\n",
            "분류 보고서:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      1.00      0.96       110\n",
            "           1       1.00      0.91      0.95       102\n",
            "\n",
            "    accuracy                           0.96       212\n",
            "   macro avg       0.96      0.96      0.96       212\n",
            "weighted avg       0.96      0.96      0.96       212\n",
            "\n",
            "모델 저장 완료: 'pose_classifier_rf.pkl'\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import joblib  # scikit-learn 모델 저장용\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# --- 메인 실행 로직 ---\n",
        "if __name__ == \"__main__\":\n",
        "    X_DATA_PATH = 'X_train.npy'; Y_DATA_PATH = 'Y_train.npy'\n",
        "    X_TEST_PATH = 'X_val.npy'; Y_TEST_PATH = 'Y_val.npy'\n",
        "\n",
        "    if not os.path.exists(X_DATA_PATH):\n",
        "        print(\"데이터 파일 없음\"); exit()\n",
        "\n",
        "    try:\n",
        "        raw_landmarks = np.load(X_DATA_PATH)\n",
        "        labels = np.load(Y_DATA_PATH)\n",
        "        raw_test = np.load(X_TEST_PATH)\n",
        "        labels_test = np.load(Y_TEST_PATH)\n",
        "\n",
        "        # 데이터 처리 함수\n",
        "        def process_data(raw_x, raw_y):\n",
        "            processed_x, processed_y = [], []\n",
        "            for i, lm in enumerate(raw_x):\n",
        "                fv = feature_engineering(lm)\n",
        "                if fv is not None:\n",
        "                    processed_x.append(fv)\n",
        "                    processed_y.append(raw_y[i])\n",
        "            return np.array(processed_x), np.array(processed_y)\n",
        "\n",
        "        print(\"데이터 전처리 중...\")\n",
        "        X_train, y_train = process_data(raw_landmarks, labels)\n",
        "        X_test, y_test = process_data(raw_test, labels_test)\n",
        "\n",
        "        # --- 셔플링 (Shuffling) ---\n",
        "        print(\"데이터 셔플링 중...\")\n",
        "        indices = np.arange(X_train.shape[0])\n",
        "        np.random.shuffle(indices)\n",
        "        X_train = X_train[indices]\n",
        "        y_train = y_train[indices]\n",
        "\n",
        "        # --- Random Forest 모델 학습 ---\n",
        "        print(f\"\\n[Random Forest] 학습 시작 (데이터 개수: {len(X_train)}개)...\")\n",
        "        # n_estimators: 트리의 개수 (보통 100~500)\n",
        "        rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "        rf_model.fit(X_train, y_train)\n",
        "        print(\"학습 완료.\")\n",
        "\n",
        "        # --- 모델 평가 ---\n",
        "        y_pred = rf_model.predict(X_test)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "        print(f\"\\n테스트 데이터 평가 결과:\")\n",
        "        print(f\"Accuracy (정확도): {accuracy:.4f}\")\n",
        "        print(\"\\n분류 보고서:\")\n",
        "        print(classification_report(y_test, y_pred))\n",
        "\n",
        "        # --- 모델 저장 ---\n",
        "        joblib.dump(rf_model, \"pose_classifier_rf.pkl\")\n",
        "        print(\"모델 저장 완료: 'pose_classifier_rf.pkl'\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"오류 발생: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Support Vector Machine (val_accuracy = 0.9271)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqFSiCEcgOkM",
        "outputId": "e75bf063-8df6-4513-ff94-b462ff11ff44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "데이터 전처리 중 (신뢰도 포함)...\n",
            "특징 벡터 차원: 20 (예상: 6+9+6+2 = 23 또는 유사)\n",
            "데이터 셔플링 중...\n",
            "\n",
            "[SVM] 학습 준비 (데이터 개수: 2139개)...\n",
            "교차 검증(Cross Validation) 진행 중...\n",
            " >> 5-Fold 검증 정확도: [0.93457944 0.93224299 0.9088785  0.92056075 0.93911007]\n",
            " >> 평균 검증 정확도: 0.9271\n",
            "최종 모델 학습 중...\n",
            "\n",
            "테스트 데이터 평가 결과:\n",
            "Accuracy (정확도): 0.4720\n",
            "\n",
            "분류 보고서:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.01      0.01       133\n",
            "           1       0.47      1.00      0.64       117\n",
            "\n",
            "    accuracy                           0.47       250\n",
            "   macro avg       0.73      0.50      0.33       250\n",
            "weighted avg       0.75      0.47      0.31       250\n",
            "\n",
            "모델 저장 완료: 'pose_classifier_svm.pkl'\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import joblib\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "\n",
        "# --- 메인 실행 로직 ---\n",
        "if __name__ == \"__main__\":\n",
        "    X_DATA_PATH = 'X_train.npy'; Y_DATA_PATH = 'Y_train.npy'\n",
        "    X_TEST_PATH = 'X_test.npy'; Y_TEST_PATH = 'Y_test.npy'\n",
        "\n",
        "    if not os.path.exists(X_DATA_PATH):\n",
        "        print(\"데이터 파일 없음\"); exit()\n",
        "\n",
        "    try:\n",
        "        raw_landmarks = np.load(X_DATA_PATH)\n",
        "        labels = np.load(Y_DATA_PATH)\n",
        "        raw_test = np.load(X_TEST_PATH)\n",
        "        labels_test = np.load(Y_TEST_PATH)\n",
        "\n",
        "        def process_data(raw_x, raw_y):\n",
        "            processed_x, processed_y = [], []\n",
        "            for i, lm in enumerate(raw_x):\n",
        "                fv = feature_engineering(lm)\n",
        "                if fv is not None:\n",
        "                    processed_x.append(fv)\n",
        "                    processed_y.append(raw_y[i])\n",
        "            return np.array(processed_x), np.array(processed_y)\n",
        "\n",
        "        print(\"데이터 전처리 중 (신뢰도 포함)...\")\n",
        "        X_train, y_train = process_data(raw_landmarks, labels)\n",
        "        X_test, y_test = process_data(raw_test, labels_test)\n",
        "\n",
        "        print(f\"특징 벡터 차원: {X_train.shape[1]} (예상: 6+9+6+2 = 23 또는 유사)\")\n",
        "\n",
        "        # --- 셔플링 (Shuffling) ---\n",
        "        print(\"데이터 셔플링 중...\")\n",
        "        indices = np.arange(X_train.shape[0])\n",
        "        np.random.shuffle(indices)\n",
        "        X_train = X_train[indices]\n",
        "        y_train = y_train[indices]\n",
        "\n",
        "        # --- SVM 모델 정의 ---\n",
        "        print(f\"\\n[SVM] 학습 준비 (데이터 개수: {len(X_train)}개)...\")\n",
        "\n",
        "        # SVM 파이프라인 (스케일링 + SVM)\n",
        "        svm_model = make_pipeline(\n",
        "            StandardScaler(),\n",
        "            SVC(kernel='rbf', C=1.0, probability=True, random_state=42)\n",
        "        )\n",
        "\n",
        "        # --- 1. 교차 검증 (Validation) ---\n",
        "        print(\"교차 검증(Cross Validation) 진행 중...\")\n",
        "        val_scores = cross_val_score(svm_model, X_train, y_train, cv=5)\n",
        "        print(f\" >> 5-Fold 검증 정확도: {val_scores}\")\n",
        "        print(f\" >> 평균 검증 정확도: {np.mean(val_scores):.4f}\")\n",
        "\n",
        "        # --- 2. 전체 데이터로 최종 학습 ---\n",
        "        print(\"최종 모델 학습 중...\")\n",
        "        svm_model.fit(X_train, y_train)\n",
        "\n",
        "        # --- 모델 평가 ---\n",
        "        y_pred = svm_model.predict(X_test)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "        print(f\"\\n테스트 데이터 평가 결과:\")\n",
        "        print(f\"Accuracy (정확도): {accuracy:.4f}\")\n",
        "        print(\"\\n분류 보고서:\")\n",
        "        print(classification_report(y_test, y_pred))\n",
        "\n",
        "        # --- 모델 저장 ---\n",
        "        joblib.dump(svm_model, \"pose_classifier_svm.pkl\")\n",
        "        print(\"모델 저장 완료: 'pose_classifier_svm.pkl'\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"오류 발생: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Log_Regression(val_accuracy = 0.8774)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "import mediapipe as mp\n",
        "import cv2\n",
        "\n",
        "X_TRAIN_PATH = 'X_train.npy'\n",
        "Y_TRAIN_PATH = 'Y_train.npy'\n",
        "X_TEST_PATH = 'X_val.npy'\n",
        "Y_TEST_PATH = 'Y_val.npy'\n",
        "\n",
        "raw_landmarks = np.load(X_TRAIN_PATH)\n",
        "labels = np.load(Y_TRAIN_PATH)\n",
        "test_landmarks = np.load(X_TEST_PATH)\n",
        "test_labels = np.load(Y_TEST_PATH)\n",
        "\n",
        "features = []\n",
        "valid_labels = []\n",
        "test_features = []\n",
        "testlabels = []\n",
        "\n",
        "for i, lm in enumerate(raw_landmarks):\n",
        "    feature_vector = feature_engineering(lm)\n",
        "    # 특징 추출에 성공한(None이 아닌) 샘플만 사용\n",
        "    if feature_vector is not None:\n",
        "        features.append(feature_vector)\n",
        "        valid_labels.append(labels[i])\n",
        "\n",
        "for i, lm in enumerate(test_landmarks):\n",
        "    feature_vector = feature_engineering(lm)\n",
        "    # 특징 추출에 성공한(None이 아닌) 샘플만 사용\n",
        "    if feature_vector is not None:\n",
        "        test_features.append(feature_vector)\n",
        "        testlabels.append(test_labels[i])\n",
        "\n",
        "X_train = np.array(features)\n",
        "Y_train = np.array(valid_labels)\n",
        "\n",
        "X_test = np.array(test_features)\n",
        "Y_test = np.array(testlabels)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "log_reg_model = LogisticRegression(solver='liblinear', random_state=42)\n",
        "log_reg_model.fit(X_train_scaled, Y_train)\n",
        "\n",
        "print(\"✅ 모델 학습 및 환경 준비 완료\")\n",
        "\n",
        "# --- 2. [추가] 모델 검증 코드 ---\n",
        "print(\"\\n--- 🧪 모델 검증 결과 (Test Set) ---\")\n",
        "Y_pred = log_reg_model.predict(X_test_scaled)\n",
        "\n",
        "# 정확도 (Accuracy)\n",
        "accuracy = accuracy_score(Y_test, Y_pred)\n",
        "print(f\"✅ 정확도: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# 혼동 행렬 (Confusion Matrix)\n",
        "print(\"\\n[ 혼동 행렬 (Confusion Matrix) ]\")\n",
        "print(confusion_matrix(Y_test, Y_pred))\n",
        "\n",
        "# 분류 리포트 (Precision, Recall, F1-score)\n",
        "# target_names는 0번 클래스와 1번 클래스의 이름을 지정합니다.\n",
        "print(\"\\n[ 분류 리포트 (Classification Report) ]\")\n",
        "print(classification_report(Y_test, Y_pred, target_names=['Bad Posture (0)', 'Good Posture (1)']))\n",
        "print(\"-----------------------------------\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (motion_transformer)",
      "language": "python",
      "name": "motion_transformer"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.23"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
