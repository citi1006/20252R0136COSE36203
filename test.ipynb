{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "gblXfDYBbsye",
        "C9um-Rfkbx-D",
        "UcWi3mJ-V5ZN",
        "H_tJKxjaBZz5"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 0. 라이브러리 설치(세션 재시작 필요)"
      ],
      "metadata": {
        "id": "mYUIi6DbBOlH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 라이브러리 설치 (최초 1회만 실행됨)\n",
        "!pip install opencv-python mediapipe tensorflow numpy"
      ],
      "metadata": {
        "id": "GJIFJySsMFi-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1efe9625-eb31-4c88-b962-61abcc64f47c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Collecting mediapipe\n",
            "  Downloading mediapipe-0.10.21-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (25.4.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (25.9.23)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.7.2)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.7.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from mediapipe) (3.10.0)\n",
            "INFO: pip is looking at multiple versions of mediapipe to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading mediapipe-0.10.20-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n",
            "  Downloading mediapipe-0.10.18-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\n",
            "  Downloading mediapipe-0.10.15-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\n",
            "  Downloading mediapipe-0.10.14-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.12/dist-packages (from mediapipe) (4.12.0.88)\n",
            "Collecting protobuf<5,>=4.25.3 (from mediapipe)\n",
            "  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Collecting sounddevice>=0.4.4 (from mediapipe)\n",
            "  Downloading sounddevice-0.5.3-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.7.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.76.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.18.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.11.12)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.12/dist-packages (from sounddevice>=0.4.4->mediapipe) (2.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.4)\n",
            "Requirement already satisfied: scipy>=1.13 in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe) (1.16.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (4.61.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.23)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading mediapipe-0.10.14-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.7/35.7 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sounddevice-0.5.3-py3-none-any.whl (32 kB)\n",
            "Installing collected packages: protobuf, sounddevice, mediapipe\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.5\n",
            "    Uninstalling protobuf-5.29.5:\n",
            "      Successfully uninstalled protobuf-5.29.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "opentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 4.25.8 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed mediapipe-0.10.14 protobuf-4.25.8 sounddevice-0.5.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "0dcdc84ef294412a9d575110727a2303"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. 기본 설정"
      ],
      "metadata": {
        "id": "sT_nG0v4bmwV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1-1. 모델 및 피쳐 선택"
      ],
      "metadata": {
        "id": "FHrtIuOFXzkA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 선택\n",
        "\n",
        "---\n",
        "\n",
        "실험할 모델을 선택하세요"
      ],
      "metadata": {
        "id": "GPDwmE1yZ_W4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 사용 가능한 옵션: 'rf' (Random Forest), 'svm' (SVM), 'keras' (Keras DL)\n",
        "CURRENT_MODEL_TYPE = 'keras'"
      ],
      "metadata": {
        "id": "T_9n92iYYIeL"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 피쳐 선택\n",
        "\n",
        "---\n",
        "\n",
        "라이브러리 로드 및 헬퍼 함수 선언 후 실험할 피쳐 엔지니어링 셀을 실행하세요"
      ],
      "metadata": {
        "id": "nRGOqsRsbxhn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 라이브러리 로드 및 헬퍼 함수"
      ],
      "metadata": {
        "id": "PXxntf21cTUR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "import joblib\n",
        "import tensorflow as tf\n",
        "import PIL.Image\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode, b64encode\n",
        "import os\n",
        "\n",
        "def calculate_angle(a, b, c):\n",
        "    reliability = a[3] * b[3] * c[3]\n",
        "    if a[3] < 0.5 or b[3] < 0.5 or c[3] < 0.5: return None, reliability\n",
        "    a_xyz, b_xyz, c_xyz = a[:3], b[:3], c[:3]\n",
        "    try:\n",
        "        ba = a_xyz - b_xyz\n",
        "        bc = c_xyz - b_xyz\n",
        "        cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))\n",
        "        angle = np.degrees(np.arccos(np.clip(cosine_angle, -1.0, 1.0)))\n",
        "        return angle, reliability\n",
        "    except: return None, reliability\n",
        "\n",
        "def calculate_plane_normal(a, b, c):\n",
        "    reliability = a[3] * b[3] * c[3]\n",
        "    if a[3] < 0.5 or b[3] < 0.5 or c[3] < 0.5: return None, reliability\n",
        "    a_xyz, b_xyz, c_xyz = a[:3], b[:3], c[:3]\n",
        "    try:\n",
        "        v1 = b_xyz - a_xyz\n",
        "        v2 = c_xyz - a_xyz\n",
        "        normal = np.cross(v1, v2)\n",
        "        norm_len = np.linalg.norm(normal)\n",
        "        if norm_len == 0: return None, reliability\n",
        "        return normal / norm_len, reliability\n",
        "    except: return None, reliability\n"
      ],
      "metadata": {
        "id": "MQfr6KTccNwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Angle + Plane"
      ],
      "metadata": {
        "id": "2jv_YviCa_Z_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_engineering(landmarks):\n",
        "    \"\"\"\n",
        "    landmarks: (33, 4) shape의 numpy array [x, y, z, visibility]\n",
        "    \"\"\"\n",
        "    if landmarks is None: return None\n",
        "    NOSE = 0\n",
        "    L_EYE, R_EYE = 2, 5\n",
        "    L_EAR, R_EAR = 7, 8\n",
        "    L_SHOULDER, R_SHOULDER = 11, 12\n",
        "    L_ELBOW, R_ELBOW = 13, 14\n",
        "    L_WRIST, R_WRIST = 15, 16\n",
        "    L_HIP, R_HIP = 23, 24\n",
        "\n",
        "    angle_features, angle_rels = [], []\n",
        "    normal_features, normal_rels = [], []\n",
        "\n",
        "    angles_to_calc = [\n",
        "        (landmarks[L_SHOULDER], landmarks[L_ELBOW], landmarks[L_WRIST]),\n",
        "        (landmarks[R_SHOULDER], landmarks[R_ELBOW], landmarks[R_WRIST]),\n",
        "        (landmarks[L_ELBOW], landmarks[L_SHOULDER], landmarks[L_HIP]),\n",
        "        (landmarks[R_ELBOW], landmarks[R_SHOULDER], landmarks[R_HIP]),\n",
        "        (landmarks[L_EAR], landmarks[L_SHOULDER], landmarks[L_HIP]),\n",
        "        (landmarks[R_EAR], landmarks[R_SHOULDER], landmarks[R_HIP])\n",
        "    ]\n",
        "    for p1, p2, p3 in angles_to_calc:\n",
        "        ang, rel = calculate_angle(p1, p2, p3)\n",
        "        angle_features.append(ang)\n",
        "        angle_rels.append(rel)\n",
        "\n",
        "    face_normal, face_rel = calculate_plane_normal(landmarks[NOSE], landmarks[L_EYE], landmarks[R_EYE])\n",
        "    normal_features.append(face_normal)\n",
        "    normal_rels.append(face_rel)\n",
        "\n",
        "    sh_nose_normal, sh_nose_rel = calculate_plane_normal(landmarks[L_SHOULDER], landmarks[R_SHOULDER], landmarks[NOSE])\n",
        "    normal_features.append(sh_nose_normal)\n",
        "    normal_rels.append(sh_nose_rel)\n",
        "\n",
        "    norm_angles_vec = [(val / 360.0) if val is not None else 0.0 for val in angle_features]\n",
        "    flat_normals_vec = []\n",
        "    for vec in normal_features:\n",
        "        if vec is not None: flat_normals_vec.extend(vec)\n",
        "        else: flat_normals_vec.extend([0.0, 0.0, 0.0])\n",
        "\n",
        "    final_features = norm_angles_vec + flat_normals_vec + angle_rels + normal_rels\n",
        "    return np.array(final_features)\n",
        "\n",
        "# 체크포인트 경로\n",
        "MODEL_PATHS = {\n",
        "    'rf': 'pose_classifier_rf.pkl',\n",
        "    'svm': 'pose_classifier_svm.pkl',\n",
        "    'keras': 'pose_classifier_keras.h5'\n",
        "}"
      ],
      "metadata": {
        "id": "nXE1PVJXbw4s"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Only Plane"
      ],
      "metadata": {
        "id": "fgzhg5nZb2zY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_engineering(landmarks):\n",
        "    \"\"\"\n",
        "    landmarks: (33, 4) shape의 numpy array [x, y, z, visibility]\n",
        "    \"\"\"\n",
        "    if landmarks is None: return None\n",
        "    NOSE = 0\n",
        "    L_EYE, R_EYE = 2, 5\n",
        "    L_EAR, R_EAR = 7, 8\n",
        "    L_SHOULDER, R_SHOULDER = 11, 12\n",
        "    L_ELBOW, R_ELBOW = 13, 14\n",
        "    L_WRIST, R_WRIST = 15, 16\n",
        "    L_HIP, R_HIP = 23, 24\n",
        "\n",
        "    normal_features, normal_rels = [], []\n",
        "\n",
        "    face_normal, face_rel = calculate_plane_normal(landmarks[NOSE], landmarks[L_EYE], landmarks[R_EYE])\n",
        "    normal_features.append(face_normal)\n",
        "    normal_rels.append(face_rel)\n",
        "\n",
        "    sh_nose_normal, sh_nose_rel = calculate_plane_normal(landmarks[L_SHOULDER], landmarks[R_SHOULDER], landmarks[NOSE])\n",
        "    normal_features.append(sh_nose_normal)\n",
        "    normal_rels.append(sh_nose_rel)\n",
        "\n",
        "    flat_normals_vec = []\n",
        "    for vec in normal_features:\n",
        "        if vec is not None: flat_normals_vec.extend(vec)\n",
        "        else: flat_normals_vec.extend([0.0, 0.0, 0.0])\n",
        "\n",
        "    final_features = flat_normals_vec + normal_rels\n",
        "    return np.array(final_features)\n",
        "\n",
        "# 체크포인트 경로\n",
        "MODEL_PATHS = {\n",
        "    'rf': 'plane_pose_classifier_rf.pkl',\n",
        "    'svm': 'plane_pose_classifier_svm.pkl',\n",
        "    'keras': 'plane_pose_classifier_keras.h5'\n",
        "}"
      ],
      "metadata": {
        "id": "TqIxhn2xb7DP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Landmark(상대값)"
      ],
      "metadata": {
        "id": "pPieh2ztgmug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def feature_engineering(landmarks):\n",
        "    \"\"\"\n",
        "    (33, 4) 랜드마크를 입력받아 다음 처리를 수행한 1차원 벡터를 반환합니다.\n",
        "\n",
        "    1. 중심 보정: 양쪽 어깨의 중점(Midpoint)을 (0,0,0)으로 이동\n",
        "    2. 크기 정규화: 어깨 너비(Distance)를 1.0으로 맞추도록 모든 좌표 스케일링\n",
        "       -> 카메라와의 거리에 상관없이 동일한 자세로 인식하게 함\n",
        "\n",
        "    Output: (132,) 형태의 1차원 벡터\n",
        "    \"\"\"\n",
        "    if landmarks is None:\n",
        "        return None\n",
        "\n",
        "    # 1. 랜드마크 복사 (원본 훼손 방지)\n",
        "    # shape: (33, 4) -> [x, y, z, visibility]\n",
        "    lm_copy = landmarks.copy()\n",
        "\n",
        "    # 인덱스 정의\n",
        "    L_SHOULDER = 11\n",
        "    R_SHOULDER = 12\n",
        "\n",
        "    # 2. 기준점(Origin) 계산: 양쪽 어깨의 중점\n",
        "    # visibility(3번 인덱스)는 계산에서 제외하고 x,y,z(0~2)만 씀\n",
        "    left_sh = lm_copy[L_SHOULDER, :3]  # [x, y, z]\n",
        "    right_sh = lm_copy[R_SHOULDER, :3] # [x, y, z]\n",
        "\n",
        "    center_point = (left_sh + right_sh) / 2.0\n",
        "\n",
        "    # 3. 위치 보정 (Translation): 모든 점을 어깨 중점이 원점이 되도록 이동\n",
        "    lm_copy[:, :3] -= center_point\n",
        "\n",
        "    # 4. 크기 보정 (Scale Normalization)\n",
        "    # 어깨 너비(유클리드 거리) 계산\n",
        "    shoulder_width = np.linalg.norm(left_sh - right_sh)\n",
        "\n",
        "    # 만약 어깨 너비가 0이거나 감지 안되면(극단적 예외) 1로 두어 나눗셈 에러 방지\n",
        "    if shoulder_width < 1e-6:\n",
        "        scale_factor = 1.0\n",
        "    else:\n",
        "        scale_factor = shoulder_width\n",
        "\n",
        "    # 모든 좌표(x,y,z)를 어깨 너비로 나눔 -> 거리에 따른 크기 변화 제거\n",
        "    lm_copy[:, :3] /= scale_factor\n",
        "\n",
        "    # 5. 평탄화 (Flatten)\n",
        "    # (33, 4) -> (132,)\n",
        "    # 이제 이 벡터는 \"카메라 위치\"나 \"거리\"와 무관한 순수한 자세 정보입니다.\n",
        "    return lm_copy.flatten()\n",
        "\n",
        "MODEL_PATHS = {\n",
        "    'rf': 'landmark_pose_classifier_rf.pkl',\n",
        "    'svm': 'landmark_pose_classifier_svm.pkl',\n",
        "    'keras': 'landmark_pose_classifier_keras.h5'\n",
        "}"
      ],
      "metadata": {
        "id": "UUJSas4sgti3"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Landmark + Plane + Angle"
      ],
      "metadata": {
        "id": "xv79WLSyuKIX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# 헬퍼 함수들은 외부에 정의되어 있다고 가정 (calculate_angle, calculate_plane_normal)\n",
        "# 만약 없다면 맨 처음 드린 코드의 헬퍼 함수를 그대로 쓰시면 됩니다.\n",
        "\n",
        "def feature_engineering(landmarks):\n",
        "    \"\"\"\n",
        "    [Hybrid Feature Engineering]\n",
        "    1. Raw 좌표: 어깨 중심 + 어깨 너비 정규화 (Scale Invariant)\n",
        "    2. 기하학적 특징: 각도(0~1 정규화) + 법선 벡터\n",
        "    3. 신뢰도 정보 포함\n",
        "\n",
        "    모든 피처의 스케일을 대략 -1~1 또는 0~1로 맞춰서 결합합니다.\n",
        "    \"\"\"\n",
        "    if landmarks is None: return None\n",
        "\n",
        "    # ==========================================\n",
        "    # 1. Raw Coordinates Processing (어깨 중심 & 크기 보정)\n",
        "    # ==========================================\n",
        "    lm_copy = landmarks.copy()\n",
        "\n",
        "    # 인덱스\n",
        "    L_SHOULDER, R_SHOULDER = 11, 12\n",
        "\n",
        "    # 기준점(Origin): 어깨 중점\n",
        "    left_sh = lm_copy[L_SHOULDER, :3]\n",
        "    right_sh = lm_copy[R_SHOULDER, :3]\n",
        "    center_point = (left_sh + right_sh) / 2.0\n",
        "\n",
        "    # 크기 기준(Scale): 어깨 너비\n",
        "    shoulder_width = np.linalg.norm(left_sh - right_sh)\n",
        "    scale_factor = shoulder_width if shoulder_width > 1e-6 else 1.0\n",
        "\n",
        "    # (A) 위치 이동 및 스케일링 -> 결과범위: 대략 -1.0 ~ 1.0\n",
        "    lm_copy[:, :3] -= center_point\n",
        "    lm_copy[:, :3] /= scale_factor\n",
        "\n",
        "    # 1차원으로 펴기 (Visibility 포함 132차원)\n",
        "    # [Tip] 만약 좌표의 visibility를 굳이 또 넣을 필요 없다면 [:, :3]만 써도 됩니다.\n",
        "    # 여기서는 정보 손실 방지를 위해 다 넣겠습니다.\n",
        "    raw_features = lm_copy.flatten()\n",
        "\n",
        "    # ==========================================\n",
        "    # 2. Geometric Features (각도 & 법선)\n",
        "    # ==========================================\n",
        "    # 인덱스 정의\n",
        "    NOSE = 0; L_EYE, R_EYE = 2, 5; L_EAR, R_EAR = 7, 8\n",
        "    L_ELBOW, R_ELBOW = 13, 14; L_WRIST, R_WRIST = 15, 16\n",
        "    L_HIP, R_HIP = 23, 24\n",
        "\n",
        "    angle_feats = []\n",
        "    normal_feats = []\n",
        "    reliability_feats = []\n",
        "\n",
        "    # (B) 각도 계산\n",
        "    angles_to_calc = [\n",
        "        (landmarks[L_SHOULDER], landmarks[L_ELBOW], landmarks[L_WRIST]),\n",
        "        (landmarks[R_SHOULDER], landmarks[R_ELBOW], landmarks[R_WRIST]),\n",
        "        (landmarks[L_ELBOW], landmarks[L_SHOULDER], landmarks[L_HIP]),\n",
        "        (landmarks[R_ELBOW], landmarks[R_SHOULDER], landmarks[R_HIP]),\n",
        "        (landmarks[L_EAR], landmarks[L_SHOULDER], landmarks[L_HIP]), # 거북목 확인용\n",
        "        (landmarks[R_EAR], landmarks[R_SHOULDER], landmarks[R_HIP])\n",
        "    ]\n",
        "\n",
        "    for p1, p2, p3 in angles_to_calc:\n",
        "        ang, rel = calculate_angle(p1, p2, p3)\n",
        "        # [중요] 각도 정규화: 0~180 -> 0.0~1.0\n",
        "        if ang is not None:\n",
        "            angle_feats.append(ang / 180.0)\n",
        "        else:\n",
        "            angle_feats.append(0.0)\n",
        "        reliability_feats.append(rel)\n",
        "\n",
        "    # (C) 법선 벡터 계산 (이미 -1~1 범위)\n",
        "    # Face Normal\n",
        "    face_norm, face_rel = calculate_plane_normal(landmarks[NOSE], landmarks[L_EYE], landmarks[R_EYE])\n",
        "    if face_norm is not None: normal_feats.extend(face_norm)\n",
        "    else: normal_feats.extend([0.0, 0.0, 0.0])\n",
        "    reliability_feats.append(face_rel)\n",
        "\n",
        "    # Chest/Shoulder Normal\n",
        "    chest_norm, chest_rel = calculate_plane_normal(landmarks[L_SHOULDER], landmarks[R_SHOULDER], landmarks[NOSE])\n",
        "    if chest_norm is not None: normal_feats.extend(chest_norm)\n",
        "    else: normal_feats.extend([0.0, 0.0, 0.0])\n",
        "    reliability_feats.append(chest_rel)\n",
        "\n",
        "    # ==========================================\n",
        "    # 3. Feature Concatenation (결합)\n",
        "    # ==========================================\n",
        "    # 리스트들을 numpy array로 변환\n",
        "    geo_vector = np.array(angle_feats + normal_feats + reliability_feats)\n",
        "\n",
        "    # [최종 결합] Raw 좌표 벡터 + 기하학적 특징 벡터\n",
        "    # 차원 수: 132 (Raw) + 6 (Angles) + 6 (Normals) + 8 (Reliability) = 152차원\n",
        "    final_features = np.concatenate([raw_features, geo_vector])\n",
        "\n",
        "    return final_features\n",
        "\n",
        "MODEL_PATHS = {\n",
        "    'rf': 'All_pose_classifier_rf.pkl',\n",
        "    'svm': 'All_pose_classifier_svm.pkl',\n",
        "    'keras': 'All_pose_classifier_keras.h5'\n",
        "}"
      ],
      "metadata": {
        "id": "CjVBkjGfuOQ5"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1-2. 모델 로드"
      ],
      "metadata": {
        "id": "C9um-Rfkbx-D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_pose_model(model_type, paths):\n",
        "    path = paths.get(model_type)\n",
        "\n",
        "    if not os.path.exists(path):\n",
        "        print(f\"오류: '{path}' 파일이 없습니다. 경로를 확인하세요.\")\n",
        "        return None\n",
        "\n",
        "    if model_type in ['rf', 'svm']:\n",
        "        # Scikit-learn 계열 (joblib 사용)\n",
        "        model = joblib.load(path)\n",
        "        print(f\"[{model_type.upper()}] Scikit-learn 모델 로드 완료!\")\n",
        "        return model\n",
        "\n",
        "    elif model_type == 'keras':\n",
        "        # Deep Learning 계열 (Keras 사용)\n",
        "        model = tf.keras.models.load_model(path)\n",
        "        print(f\"[{model_type.upper()}] Keras 모델 로드 완료!\")\n",
        "        return model\n",
        "\n",
        "    else:\n",
        "        print(\"지원하지 않는 모델 타입입니다.\")\n",
        "        return None\n",
        "\n",
        "# 모델 로드 실행\n",
        "model = load_pose_model(CURRENT_MODEL_TYPE, MODEL_PATHS)\n",
        "\n",
        "# ==========================================\n",
        "#  예측 수행 함수\n",
        "# ==========================================\n",
        "def predict_pose(model, features, model_type):\n",
        "    \"\"\"\n",
        "    모델 타입에 따라 알맞은 전처리와 예측 결과를 반환합니다.\n",
        "    \"\"\"\n",
        "    if model is None or features is None:\n",
        "        return None\n",
        "\n",
        "    # 1) Scikit-learn 계열 (RF, SVM)\n",
        "    if model_type in ['rf', 'svm']:\n",
        "        # 입력: 2차원 배열 필요 (1, N) -> reshape(1, -1) 권장\n",
        "        input_data = features.reshape(1, -1)\n",
        "        prediction = model.predict(input_data)[0] # 결과가 배열로 나오므로 첫 번째 요소 선택\n",
        "        return prediction\n",
        "\n",
        "    # 2) Keras 계열 (Deep Learning)\n",
        "    elif model_type == 'keras':\n",
        "        # 입력: 반드시 (1, features_len) 형태여야 함\n",
        "        input_data = features.reshape(1, -1)\n",
        "\n",
        "        # 예측: 확률 분포 반환 (예: [0.1, 0.8, 0.1])\n",
        "        prediction_probs = model.predict(input_data, verbose=0)\n",
        "\n",
        "        # 가장 높은 확률의 인덱스 추출\n",
        "        prediction = np.argmax(prediction_probs)\n",
        "        return prediction\n",
        "\n",
        "# ==========================================\n",
        "# MediaPipe 설정\n",
        "# ==========================================\n",
        "mp_pose = mp.solutions.pose\n",
        "pose = mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "\n",
        "print(f\"\\n현재 설정된 모델: {CURRENT_MODEL_TYPE}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6H9X1zfV0aN",
        "outputId": "e0d5ff14-fc84-49b4-bce1-c695dd6e1ea0"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[KERAS] Keras 모델 로드 완료!\n",
            "\n",
            "현재 설정된 모델: keras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. 웹캠 통신 함수"
      ],
      "metadata": {
        "id": "H_tJKxjaBZz5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def js_to_image(js_reply):\n",
        "  \"\"\"JS 객체를 OpenCV 이미지로 변환\"\"\"\n",
        "  image_bytes = b64decode(js_reply.split(',')[1])\n",
        "  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "  img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "  return img\n",
        "\n",
        "def bbox_to_bytes(bbox_array):\n",
        "  \"\"\"OpenCV 이미지를 JS 전송용 byte로 변환\"\"\"\n",
        "  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n",
        "  iobuf = io.BytesIO()\n",
        "  bbox_PIL.save(iobuf, format='png')\n",
        "  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "  return bbox_bytes\n",
        "\n",
        "def video_stream():\n",
        "  js = Javascript('''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "    var captureCanvas;\n",
        "    var imgElement;\n",
        "    var labelElement;\n",
        "\n",
        "    var pendingResolve = null;\n",
        "    var shutdown = false;\n",
        "\n",
        "    function removeDom() {\n",
        "       stream.getVideoTracks()[0].stop();\n",
        "       video.remove();\n",
        "       div.remove();\n",
        "       video = null;\n",
        "       div = null;\n",
        "       stream = null;\n",
        "       imgElement = null;\n",
        "       captureCanvas = null;\n",
        "       labelElement = null;\n",
        "    }\n",
        "\n",
        "    function onAnimationFrame() {\n",
        "      if (!shutdown) {\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "      }\n",
        "      if (pendingResolve) {\n",
        "        var result = \"\";\n",
        "        if (!shutdown) {\n",
        "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
        "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
        "        }\n",
        "        var lp = pendingResolve;\n",
        "        pendingResolve = null;\n",
        "        lp(result);\n",
        "      }\n",
        "    }\n",
        "\n",
        "    async function createDom() {\n",
        "      if (div !== null) {\n",
        "        return stream;\n",
        "      }\n",
        "      div = document.createElement('div');\n",
        "      div.style.border = '2px solid black';\n",
        "      div.style.padding = '3px';\n",
        "      div.style.width = '100%';\n",
        "      div.style.maxWidth = '600px';\n",
        "      document.body.appendChild(div);\n",
        "\n",
        "      const modelOut = document.createElement('div');\n",
        "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
        "      labelElement = document.createElement('span');\n",
        "      labelElement.innerText = 'No data';\n",
        "      labelElement.style.fontWeight = 'bold';\n",
        "      modelOut.appendChild(labelElement);\n",
        "      div.appendChild(modelOut);\n",
        "\n",
        "      video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      video.width = div.clientWidth - 6;\n",
        "      video.setAttribute('playsinline', '');\n",
        "      video.onclick = () => { shutdown = true; };\n",
        "      stream = await navigator.mediaDevices.getUserMedia(\n",
        "          {video: { facingMode: \"user\"}});\n",
        "      div.appendChild(video);\n",
        "\n",
        "      imgElement = document.createElement('img');\n",
        "      imgElement.style.position = 'absolute';\n",
        "      imgElement.style.zIndex = 1;\n",
        "      imgElement.onclick = () => { shutdown = true; };\n",
        "      div.appendChild(imgElement);\n",
        "\n",
        "      const instruction = document.createElement('div');\n",
        "      instruction.innerHTML =\n",
        "          '<span style=\"color: red; font-weight: bold;\">' +\n",
        "          'When finished, click here or on the video to stop this demo</span>';\n",
        "      div.appendChild(instruction);\n",
        "      instruction.onclick = () => { shutdown = true; };\n",
        "\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      captureCanvas = document.createElement('canvas');\n",
        "      captureCanvas.width = 640;\n",
        "      captureCanvas.height = 480;\n",
        "      window.requestAnimationFrame(onAnimationFrame);\n",
        "\n",
        "      return stream;\n",
        "    }\n",
        "    async function stream_frame(label, imgData) {\n",
        "      if (shutdown) {\n",
        "        removeDom();\n",
        "        shutdown = false;\n",
        "        return '';\n",
        "      }\n",
        "      var preCreate = Date.now();\n",
        "      stream = await createDom();\n",
        "\n",
        "      var preShow = Date.now();\n",
        "      if (label != \"\") {\n",
        "        labelElement.innerHTML = label;\n",
        "      }\n",
        "      if (imgData != \"\") {\n",
        "        var videoRect = video.getClientRects()[0];\n",
        "        imgElement.style.top = videoRect.top + \"px\";\n",
        "        imgElement.style.left = videoRect.left + \"px\";\n",
        "        imgElement.style.width = videoRect.width + \"px\";\n",
        "        imgElement.style.height = videoRect.height + \"px\";\n",
        "        imgElement.src = imgData;\n",
        "      }\n",
        "      var preCapture = Date.now();\n",
        "      var result = await new Promise(function(resolve, reject) {\n",
        "        pendingResolve = resolve;\n",
        "      });\n",
        "      shutdown = false;\n",
        "\n",
        "      return {'create': preShow - preCreate,\n",
        "              'show': preCapture - preShow,\n",
        "              'capture': Date.now() - preCapture,\n",
        "              'img': result};\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "\n",
        "def video_frame(label, bbox):\n",
        "  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n",
        "  return data"
      ],
      "metadata": {
        "id": "C550Qk1sA7Qq"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. 메인 실행 (영상)"
      ],
      "metadata": {
        "id": "ccKVoOy-BhT0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_TEST_PATH = 'X_test.npy'\n",
        "Y_TEST_PATH = 'Y_test.npy'\n",
        "raw_test = np.load(X_TEST_PATH)\n",
        "labels_test = np.load(Y_TEST_PATH)\n",
        "def process_data(raw_x, raw_y):\n",
        "  processed_x, processed_y = [], []\n",
        "  for i, lm in enumerate(raw_x):\n",
        "      fv = feature_engineering(lm)\n",
        "      if fv is not None:\n",
        "          processed_x.append(fv)\n",
        "          processed_y.append(raw_y[i])\n",
        "  return np.array(processed_x), np.array(processed_y)\n",
        "X_test, y_test = process_data(raw_test, labels_test)\n",
        "\n",
        "y_pred_probs = model.predict(X_test)\n",
        "\n",
        "print(\"--- [디버깅] 차원(Shape) 확인 ---\")\n",
        "print(f\"1. 예측값(y_pred)의 형태: {y_pred_probs.shape}\") # 예: (100, 1) 이어야 함\n",
        "print(f\"2. 정답값(y_test)의 형태: {y_test.shape}\")      # 예: (100,) 또는 (100, 1) 이어야 함\n",
        "\n",
        "print(\"\\n--- [디버깅] 실제 값 비교 (상위 5개) ---\")\n",
        "for i in range(len(y_test)):\n",
        "    print(f\"샘플 {i}: 예측확률={y_pred_probs[i][0]:.4f} vs 정답레이블={y_test[i]}\")\n",
        "\n",
        "print(\"\\n--- [디버깅] 레이블 데이터 점검 ---\")\n",
        "unique_labels = np.unique(y_test)\n",
        "print(f\"y_test에 들어있는 고유값들: {unique_labels}\")\n",
        "# 만약 여기에 [0, 1] 이외의 값(예: 1, 2 또는 문자열)이 있다면 정확도가 0이 나옵니다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6YWiiXUu3Bi",
        "outputId": "5b5cf8bb-43ef-4fad-cb93-5e37a92bbbec"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "--- [디버깅] 차원(Shape) 확인 ---\n",
            "1. 예측값(y_pred)의 형태: (250, 1)\n",
            "2. 정답값(y_test)의 형태: (250,)\n",
            "\n",
            "--- [디버깅] 실제 값 비교 (상위 5개) ---\n",
            "샘플 0: 예측확률=0.9599 vs 정답레이블=1\n",
            "샘플 1: 예측확률=0.9747 vs 정답레이블=1\n",
            "샘플 2: 예측확률=0.9842 vs 정답레이블=1\n",
            "샘플 3: 예측확률=0.9641 vs 정답레이블=1\n",
            "샘플 4: 예측확률=0.9644 vs 정답레이블=1\n",
            "샘플 5: 예측확률=0.9699 vs 정답레이블=1\n",
            "샘플 6: 예측확률=0.9716 vs 정답레이블=1\n",
            "샘플 7: 예측확률=0.9686 vs 정답레이블=1\n",
            "샘플 8: 예측확률=0.9829 vs 정답레이블=1\n",
            "샘플 9: 예측확률=0.9821 vs 정답레이블=1\n",
            "샘플 10: 예측확률=0.9596 vs 정답레이블=1\n",
            "샘플 11: 예측확률=0.9316 vs 정답레이블=1\n",
            "샘플 12: 예측확률=0.9748 vs 정답레이블=1\n",
            "샘플 13: 예측확률=0.9834 vs 정답레이블=1\n",
            "샘플 14: 예측확률=0.9848 vs 정답레이블=1\n",
            "샘플 15: 예측확률=0.9323 vs 정답레이블=1\n",
            "샘플 16: 예측확률=0.9693 vs 정답레이블=1\n",
            "샘플 17: 예측확률=0.9514 vs 정답레이블=1\n",
            "샘플 18: 예측확률=0.9837 vs 정답레이블=1\n",
            "샘플 19: 예측확률=0.9695 vs 정답레이블=1\n",
            "샘플 20: 예측확률=0.9559 vs 정답레이블=1\n",
            "샘플 21: 예측확률=0.9651 vs 정답레이블=1\n",
            "샘플 22: 예측확률=0.9738 vs 정답레이블=1\n",
            "샘플 23: 예측확률=0.9689 vs 정답레이블=1\n",
            "샘플 24: 예측확률=0.9701 vs 정답레이블=1\n",
            "샘플 25: 예측확률=0.9805 vs 정답레이블=1\n",
            "샘플 26: 예측확률=0.9852 vs 정답레이블=1\n",
            "샘플 27: 예측확률=0.9787 vs 정답레이블=1\n",
            "샘플 28: 예측확률=0.9738 vs 정답레이블=1\n",
            "샘플 29: 예측확률=0.9688 vs 정답레이블=1\n",
            "샘플 30: 예측확률=0.9540 vs 정답레이블=1\n",
            "샘플 31: 예측확률=0.9687 vs 정답레이블=1\n",
            "샘플 32: 예측확률=0.9629 vs 정답레이블=1\n",
            "샘플 33: 예측확률=0.9722 vs 정답레이블=1\n",
            "샘플 34: 예측확률=0.9650 vs 정답레이블=1\n",
            "샘플 35: 예측확률=0.9773 vs 정답레이블=1\n",
            "샘플 36: 예측확률=0.9178 vs 정답레이블=1\n",
            "샘플 37: 예측확률=0.9685 vs 정답레이블=1\n",
            "샘플 38: 예측확률=0.9493 vs 정답레이블=1\n",
            "샘플 39: 예측확률=0.9797 vs 정답레이블=1\n",
            "샘플 40: 예측확률=0.9752 vs 정답레이블=1\n",
            "샘플 41: 예측확률=0.9647 vs 정답레이블=1\n",
            "샘플 42: 예측확률=0.9723 vs 정답레이블=1\n",
            "샘플 43: 예측확률=0.9647 vs 정답레이블=1\n",
            "샘플 44: 예측확률=0.9397 vs 정답레이블=1\n",
            "샘플 45: 예측확률=0.9839 vs 정답레이블=1\n",
            "샘플 46: 예측확률=0.9671 vs 정답레이블=1\n",
            "샘플 47: 예측확률=0.9634 vs 정답레이블=1\n",
            "샘플 48: 예측확률=0.9809 vs 정답레이블=1\n",
            "샘플 49: 예측확률=0.9568 vs 정답레이블=1\n",
            "샘플 50: 예측확률=0.9741 vs 정답레이블=1\n",
            "샘플 51: 예측확률=0.9768 vs 정답레이블=1\n",
            "샘플 52: 예측확률=0.9636 vs 정답레이블=1\n",
            "샘플 53: 예측확률=0.9771 vs 정답레이블=1\n",
            "샘플 54: 예측확률=0.9871 vs 정답레이블=1\n",
            "샘플 55: 예측확률=0.9726 vs 정답레이블=1\n",
            "샘플 56: 예측확률=0.9774 vs 정답레이블=1\n",
            "샘플 57: 예측확률=0.9809 vs 정답레이블=1\n",
            "샘플 58: 예측확률=0.9707 vs 정답레이블=1\n",
            "샘플 59: 예측확률=0.9680 vs 정답레이블=1\n",
            "샘플 60: 예측확률=0.9251 vs 정답레이블=1\n",
            "샘플 61: 예측확률=0.9706 vs 정답레이블=1\n",
            "샘플 62: 예측확률=0.9420 vs 정답레이블=1\n",
            "샘플 63: 예측확률=0.9580 vs 정답레이블=1\n",
            "샘플 64: 예측확률=0.9781 vs 정답레이블=1\n",
            "샘플 65: 예측확률=0.9523 vs 정답레이블=1\n",
            "샘플 66: 예측확률=0.9604 vs 정답레이블=1\n",
            "샘플 67: 예측확률=0.9790 vs 정답레이블=1\n",
            "샘플 68: 예측확률=0.9579 vs 정답레이블=1\n",
            "샘플 69: 예측확률=0.9853 vs 정답레이블=1\n",
            "샘플 70: 예측확률=0.9812 vs 정답레이블=1\n",
            "샘플 71: 예측확률=0.9781 vs 정답레이블=1\n",
            "샘플 72: 예측확률=0.9787 vs 정답레이블=1\n",
            "샘플 73: 예측확률=0.9738 vs 정답레이블=1\n",
            "샘플 74: 예측확률=0.9756 vs 정답레이블=1\n",
            "샘플 75: 예측확률=0.9666 vs 정답레이블=1\n",
            "샘플 76: 예측확률=0.9719 vs 정답레이블=1\n",
            "샘플 77: 예측확률=0.9782 vs 정답레이블=1\n",
            "샘플 78: 예측확률=0.9737 vs 정답레이블=1\n",
            "샘플 79: 예측확률=0.9743 vs 정답레이블=1\n",
            "샘플 80: 예측확률=0.9761 vs 정답레이블=1\n",
            "샘플 81: 예측확률=0.9764 vs 정답레이블=1\n",
            "샘플 82: 예측확률=0.9690 vs 정답레이블=1\n",
            "샘플 83: 예측확률=0.9669 vs 정답레이블=1\n",
            "샘플 84: 예측확률=0.9759 vs 정답레이블=1\n",
            "샘플 85: 예측확률=0.9474 vs 정답레이블=1\n",
            "샘플 86: 예측확률=0.9443 vs 정답레이블=1\n",
            "샘플 87: 예측확률=0.9763 vs 정답레이블=1\n",
            "샘플 88: 예측확률=0.9820 vs 정답레이블=1\n",
            "샘플 89: 예측확률=0.9687 vs 정답레이블=1\n",
            "샘플 90: 예측확률=0.9737 vs 정답레이블=1\n",
            "샘플 91: 예측확률=0.9700 vs 정답레이블=1\n",
            "샘플 92: 예측확률=0.9744 vs 정답레이블=1\n",
            "샘플 93: 예측확률=0.9828 vs 정답레이블=1\n",
            "샘플 94: 예측확률=0.9695 vs 정답레이블=1\n",
            "샘플 95: 예측확률=0.9830 vs 정답레이블=1\n",
            "샘플 96: 예측확률=0.9770 vs 정답레이블=1\n",
            "샘플 97: 예측확률=0.9765 vs 정답레이블=1\n",
            "샘플 98: 예측확률=0.9864 vs 정답레이블=1\n",
            "샘플 99: 예측확률=0.9066 vs 정답레이블=1\n",
            "샘플 100: 예측확률=0.9832 vs 정답레이블=1\n",
            "샘플 101: 예측확률=0.9755 vs 정답레이블=1\n",
            "샘플 102: 예측확률=0.9519 vs 정답레이블=1\n",
            "샘플 103: 예측확률=0.9754 vs 정답레이블=1\n",
            "샘플 104: 예측확률=0.9757 vs 정답레이블=1\n",
            "샘플 105: 예측확률=0.9655 vs 정답레이블=1\n",
            "샘플 106: 예측확률=0.9668 vs 정답레이블=1\n",
            "샘플 107: 예측확률=0.9804 vs 정답레이블=1\n",
            "샘플 108: 예측확률=0.9731 vs 정답레이블=1\n",
            "샘플 109: 예측확률=0.9797 vs 정답레이블=1\n",
            "샘플 110: 예측확률=0.9564 vs 정답레이블=1\n",
            "샘플 111: 예측확률=0.9559 vs 정답레이블=1\n",
            "샘플 112: 예측확률=0.9626 vs 정답레이블=1\n",
            "샘플 113: 예측확률=0.9763 vs 정답레이블=1\n",
            "샘플 114: 예측확률=0.9776 vs 정답레이블=1\n",
            "샘플 115: 예측확률=0.9757 vs 정답레이블=1\n",
            "샘플 116: 예측확률=0.9607 vs 정답레이블=1\n",
            "샘플 117: 예측확률=0.0000 vs 정답레이블=0\n",
            "샘플 118: 예측확률=0.2575 vs 정답레이블=0\n",
            "샘플 119: 예측확률=0.5305 vs 정답레이블=0\n",
            "샘플 120: 예측확률=0.0000 vs 정답레이블=0\n",
            "샘플 121: 예측확률=0.5355 vs 정답레이블=0\n",
            "샘플 122: 예측확률=0.0011 vs 정답레이블=0\n",
            "샘플 123: 예측확률=0.0000 vs 정답레이블=0\n",
            "샘플 124: 예측확률=0.0016 vs 정답레이블=0\n",
            "샘플 125: 예측확률=0.4917 vs 정답레이블=0\n",
            "샘플 126: 예측확률=0.0000 vs 정답레이블=0\n",
            "샘플 127: 예측확률=0.0000 vs 정답레이블=0\n",
            "샘플 128: 예측확률=0.0021 vs 정답레이블=0\n",
            "샘플 129: 예측확률=0.1337 vs 정답레이블=0\n",
            "샘플 130: 예측확률=0.0000 vs 정답레이블=0\n",
            "샘플 131: 예측확률=0.0173 vs 정답레이블=0\n",
            "샘플 132: 예측확률=0.0480 vs 정답레이블=0\n",
            "샘플 133: 예측확률=0.7867 vs 정답레이블=0\n",
            "샘플 134: 예측확률=0.0000 vs 정답레이블=0\n",
            "샘플 135: 예측확률=0.0000 vs 정답레이블=0\n",
            "샘플 136: 예측확률=0.7177 vs 정답레이블=0\n",
            "샘플 137: 예측확률=0.0746 vs 정답레이블=0\n",
            "샘플 138: 예측확률=0.7450 vs 정답레이블=0\n",
            "샘플 139: 예측확률=0.5069 vs 정답레이블=0\n",
            "샘플 140: 예측확률=0.0000 vs 정답레이블=0\n",
            "샘플 141: 예측확률=0.0426 vs 정답레이블=0\n",
            "샘플 142: 예측확률=0.0541 vs 정답레이블=0\n",
            "샘플 143: 예측확률=0.0000 vs 정답레이블=0\n",
            "샘플 144: 예측확률=0.0000 vs 정답레이블=0\n",
            "샘플 145: 예측확률=0.0300 vs 정답레이블=0\n",
            "샘플 146: 예측확률=0.0139 vs 정답레이블=0\n",
            "샘플 147: 예측확률=0.0871 vs 정답레이블=0\n",
            "샘플 148: 예측확률=0.0001 vs 정답레이블=0\n",
            "샘플 149: 예측확률=0.0000 vs 정답레이블=0\n",
            "샘플 150: 예측확률=0.0238 vs 정답레이블=0\n",
            "샘플 151: 예측확률=0.5477 vs 정답레이블=0\n",
            "샘플 152: 예측확률=0.0000 vs 정답레이블=0\n",
            "샘플 153: 예측확률=0.0103 vs 정답레이블=0\n",
            "샘플 154: 예측확률=0.0278 vs 정답레이블=0\n",
            "샘플 155: 예측확률=0.0000 vs 정답레이블=0\n",
            "샘플 156: 예측확률=0.0000 vs 정답레이블=0\n",
            "샘플 157: 예측확률=0.0000 vs 정답레이블=0\n",
            "샘플 158: 예측확률=0.0000 vs 정답레이블=0\n",
            "샘플 159: 예측확률=0.0000 vs 정답레이블=0\n",
            "샘플 160: 예측확률=0.0000 vs 정답레이블=0\n",
            "샘플 161: 예측확률=0.0000 vs 정답레이블=0\n",
            "샘플 162: 예측확률=0.0397 vs 정답레이블=0\n",
            "샘플 163: 예측확률=0.2114 vs 정답레이블=0\n",
            "샘플 164: 예측확률=0.0330 vs 정답레이블=0\n",
            "샘플 165: 예측확률=0.0000 vs 정답레이블=0\n",
            "샘플 166: 예측확률=0.2246 vs 정답레이블=0\n",
            "샘플 167: 예측확률=0.0000 vs 정답레이블=0\n",
            "샘플 168: 예측확률=0.0000 vs 정답레이블=0\n",
            "샘플 169: 예측확률=0.2902 vs 정답레이블=0\n",
            "샘플 170: 예측확률=0.2378 vs 정답레이블=0\n",
            "샘플 171: 예측확률=0.0705 vs 정답레이블=0\n",
            "샘플 172: 예측확률=0.0590 vs 정답레이블=0\n",
            "샘플 173: 예측확률=0.0016 vs 정답레이블=0\n",
            "샘플 174: 예측확률=0.4512 vs 정답레이블=0\n",
            "샘플 175: 예측확률=0.0000 vs 정답레이블=0\n",
            "샘플 176: 예측확률=0.0152 vs 정답레이블=0\n",
            "샘플 177: 예측확률=0.0000 vs 정답레이블=0\n",
            "샘플 178: 예측확률=0.3336 vs 정답레이블=0\n",
            "샘플 179: 예측확률=0.0501 vs 정답레이블=0\n",
            "샘플 180: 예측확률=0.3693 vs 정답레이블=0\n",
            "샘플 181: 예측확률=0.0087 vs 정답레이블=0\n",
            "샘플 182: 예측확률=0.5254 vs 정답레이블=0\n",
            "샘플 183: 예측확률=0.8970 vs 정답레이블=0\n",
            "샘플 184: 예측확률=0.6117 vs 정답레이블=0\n",
            "샘플 185: 예측확률=0.0596 vs 정답레이블=0\n",
            "샘플 186: 예측확률=0.0003 vs 정답레이블=0\n",
            "샘플 187: 예측확률=0.0117 vs 정답레이블=0\n",
            "샘플 188: 예측확률=0.8076 vs 정답레이블=0\n",
            "샘플 189: 예측확률=0.0083 vs 정답레이블=0\n",
            "샘플 190: 예측확률=0.1495 vs 정답레이블=0\n",
            "샘플 191: 예측확률=0.3408 vs 정답레이블=0\n",
            "샘플 192: 예측확률=0.5193 vs 정답레이블=0\n",
            "샘플 193: 예측확률=0.4597 vs 정답레이블=0\n",
            "샘플 194: 예측확률=0.0083 vs 정답레이블=0\n",
            "샘플 195: 예측확률=0.0015 vs 정답레이블=0\n",
            "샘플 196: 예측확률=0.1062 vs 정답레이블=0\n",
            "샘플 197: 예측확률=0.0009 vs 정답레이블=0\n",
            "샘플 198: 예측확률=0.1272 vs 정답레이블=0\n",
            "샘플 199: 예측확률=0.0000 vs 정답레이블=0\n",
            "샘플 200: 예측확률=0.5289 vs 정답레이블=0\n",
            "샘플 201: 예측확률=0.3870 vs 정답레이블=0\n",
            "샘플 202: 예측확률=0.2834 vs 정답레이블=0\n",
            "샘플 203: 예측확률=0.0469 vs 정답레이블=0\n",
            "샘플 204: 예측확률=0.0002 vs 정답레이블=0\n",
            "샘플 205: 예측확률=0.0011 vs 정답레이블=0\n",
            "샘플 206: 예측확률=0.0000 vs 정답레이블=0\n",
            "샘플 207: 예측확률=0.0013 vs 정답레이블=0\n",
            "샘플 208: 예측확률=0.0383 vs 정답레이블=0\n",
            "샘플 209: 예측확률=0.0167 vs 정답레이블=0\n",
            "샘플 210: 예측확률=0.0000 vs 정답레이블=0\n",
            "샘플 211: 예측확률=0.0757 vs 정답레이블=0\n",
            "샘플 212: 예측확률=0.0016 vs 정답레이블=0\n",
            "샘플 213: 예측확률=0.0373 vs 정답레이블=0\n",
            "샘플 214: 예측확률=0.0001 vs 정답레이블=0\n",
            "샘플 215: 예측확률=0.7835 vs 정답레이블=0\n",
            "샘플 216: 예측확률=0.6917 vs 정답레이블=0\n",
            "샘플 217: 예측확률=0.0238 vs 정답레이블=0\n",
            "샘플 218: 예측확률=0.0897 vs 정답레이블=0\n",
            "샘플 219: 예측확률=0.7795 vs 정답레이블=0\n",
            "샘플 220: 예측확률=0.0092 vs 정답레이블=0\n",
            "샘플 221: 예측확률=0.0001 vs 정답레이블=0\n",
            "샘플 222: 예측확률=0.1751 vs 정답레이블=0\n",
            "샘플 223: 예측확률=0.0000 vs 정답레이블=0\n",
            "샘플 224: 예측확률=0.0278 vs 정답레이블=0\n",
            "샘플 225: 예측확률=0.3070 vs 정답레이블=0\n",
            "샘플 226: 예측확률=0.0001 vs 정답레이블=0\n",
            "샘플 227: 예측확률=0.0212 vs 정답레이블=0\n",
            "샘플 228: 예측확률=0.3037 vs 정답레이블=0\n",
            "샘플 229: 예측확률=0.0420 vs 정답레이블=0\n",
            "샘플 230: 예측확률=0.0052 vs 정답레이블=0\n",
            "샘플 231: 예측확률=0.0000 vs 정답레이블=0\n",
            "샘플 232: 예측확률=0.0000 vs 정답레이블=0\n",
            "샘플 233: 예측확률=0.2540 vs 정답레이블=0\n",
            "샘플 234: 예측확률=0.6105 vs 정답레이블=0\n",
            "샘플 235: 예측확률=0.0064 vs 정답레이블=0\n",
            "샘플 236: 예측확률=0.0049 vs 정답레이블=0\n",
            "샘플 237: 예측확률=0.8539 vs 정답레이블=0\n",
            "샘플 238: 예측확률=0.0210 vs 정답레이블=0\n",
            "샘플 239: 예측확률=0.0000 vs 정답레이블=0\n",
            "샘플 240: 예측확률=0.6230 vs 정답레이블=0\n",
            "샘플 241: 예측확률=0.0053 vs 정답레이블=0\n",
            "샘플 242: 예측확률=0.0041 vs 정답레이블=0\n",
            "샘플 243: 예측확률=0.0306 vs 정답레이블=0\n",
            "샘플 244: 예측확률=0.2044 vs 정답레이블=0\n",
            "샘플 245: 예측확률=0.0393 vs 정답레이블=0\n",
            "샘플 246: 예측확률=0.0098 vs 정답레이블=0\n",
            "샘플 247: 예측확률=0.0012 vs 정답레이블=0\n",
            "샘플 248: 예측확률=0.7582 vs 정답레이블=0\n",
            "샘플 249: 예측확률=0.0177 vs 정답레이블=0\n",
            "\n",
            "--- [디버깅] 레이블 데이터 점검 ---\n",
            "y_test에 들어있는 고유값들: [0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 2. 예측 통합 헬퍼 함수 (핵심 모듈)\n",
        "# ==========================================\n",
        "def predict_wrapper(model, features, model_type):\n",
        "    \"\"\"\n",
        "    모델 타입에 따라 적절한 예측 함수를 호출하고,\n",
        "    (상태 문자열, 'Good'일 확률)을 반환합니다.\n",
        "    \"\"\"\n",
        "    if features is None:\n",
        "        return \"No Pose\", 0.0\n",
        "\n",
        "    # 입력 차원 맞추기 (1, N)\n",
        "    input_data = features.reshape(1, -1)\n",
        "\n",
        "    prob_good = 0.0\n",
        "\n",
        "    try:\n",
        "        # A. 머신러닝 모델 (RF, SVM)\n",
        "        if model_type in ['rf', 'svm']:\n",
        "            # predict_proba 반환값: [[prob_bad, prob_good]]\n",
        "            probs = model.predict_proba(input_data)[0]\n",
        "            prob_good = probs[1] # 1번 인덱스가 Good이라고 가정\n",
        "\n",
        "        # B. 딥러닝 모델 (Keras)\n",
        "        elif model_type == 'keras':\n",
        "            # 반환값 형태: [[0.78]] (2차원 배열 안에 값 1개)\n",
        "            pred = model.predict(input_data, verbose=0)\n",
        "\n",
        "            prob_good = pred[0][0]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Prediction Logic Error: {e}\")\n",
        "        return \"Error\", 0.0\n",
        "\n",
        "    # 상태 결정 (임계값 0.5)\n",
        "    if prob_good > 0.5:\n",
        "        return \"Good\", prob_good\n",
        "    else:\n",
        "        return \"Bad\", prob_good\n",
        "\n",
        "# ==========================================\n",
        "# 3. 실시간 웹캠 분석 루프\n",
        "# ==========================================\n",
        "# 스트리밍 시작 (Colab 환경 가정)\n",
        "try:\n",
        "    video_stream() # 이미 정의되어 있다고 가정\n",
        "except NameError:\n",
        "    print(\"video_stream() 함수가 정의되지 않았습니다. 이전 셀을 실행했는지 확인하세요.\")\n",
        "\n",
        "label_html = 'Starting Posture Analysis...'\n",
        "bbox = ''\n",
        "count = 0\n",
        "\n",
        "while True:\n",
        "    # Colab JS 프록시를 통해 프레임 받기\n",
        "    try:\n",
        "        js_reply = video_frame(label_html, bbox)\n",
        "        if not js_reply: break\n",
        "    except: break\n",
        "\n",
        "    # 1. 이미지 변환 및 전처리\n",
        "    img = js_to_image(js_reply[\"img\"])\n",
        "    img = cv2.flip(img, 1) # 거울 모드\n",
        "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # 2. MediaPipe 분석\n",
        "    results = pose.process(img_rgb)\n",
        "\n",
        "    status = \"No Pose\"\n",
        "    prediction_prob = 0.0\n",
        "    color = (200, 200, 200) # 회색 (Default)\n",
        "\n",
        "    # 3. 랜드마크 감지 시 처리\n",
        "    if results.pose_landmarks:\n",
        "        try:\n",
        "            # (1) 랜드마크 추출 및 Numpy 변환\n",
        "            landmarks_list = []\n",
        "            for lm in results.pose_landmarks.landmark:\n",
        "                landmarks_list.append([lm.x, lm.y, lm.z, lm.visibility])\n",
        "            landmarks_arr = np.array(landmarks_list)\n",
        "\n",
        "            # (2) Feature Engineering (기존 함수 사용)\n",
        "            features = feature_engineering(landmarks_arr)\n",
        "\n",
        "            if features is not None:\n",
        "                # (3) [모듈화된 예측 함수 호출]\n",
        "                # 여기서 모델 타입에 따라 알아서 처리됩니다.\n",
        "                status, prediction_prob = predict_wrapper(model, features, CURRENT_MODEL_TYPE)\n",
        "\n",
        "                # 색상 설정\n",
        "                if status == \"Good\":\n",
        "                    color = (0, 255, 0) # 초록\n",
        "                elif status == \"Bad\":\n",
        "                    color = (0, 0, 255) # 빨강\n",
        "                else:\n",
        "                    color = (0, 255, 255) # 노랑 (에러 등)\n",
        "\n",
        "                label_html = f\"[{CURRENT_MODEL_TYPE.upper()}] Status: {status} ({prediction_prob*100:.1f}%)\"\n",
        "\n",
        "            # (4) 시각화: 랜드마크 그리기\n",
        "            mp_drawing.draw_landmarks(\n",
        "                img,\n",
        "                results.pose_landmarks,\n",
        "                mp_pose.POSE_CONNECTIONS,\n",
        "                mp_drawing.DrawingSpec(color=color, thickness=2, circle_radius=2),\n",
        "                mp_drawing.DrawingSpec(color=(255,255,255), thickness=2, circle_radius=2)\n",
        "            )\n",
        "\n",
        "            # (5) 시각화: 텍스트 오버레이\n",
        "            cv2.rectangle(img, (0,0), (300, 60), (245, 117, 16), -1)\n",
        "            cv2.putText(img, f\"{status} {prediction_prob*100:.0f}%\", (10, 40),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
        "            cv2.putText(img, f\"Model: {CURRENT_MODEL_TYPE}\", (10, 15), # 현재 모델 표시\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 200), 1)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Main Loop Error: {e}\")\n",
        "            pass\n",
        "    else:\n",
        "        label_html = 'Pose Not Detected'\n",
        "\n",
        "    # 4. 출력용 이미지 생성 (RGBA)\n",
        "    img_rgba = cv2.cvtColor(img, cv2.COLOR_BGR2RGBA)\n",
        "    bbox_bytes = bbox_to_bytes(img_rgba)\n",
        "    bbox = bbox_bytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "on6u6v32A94q",
        "outputId": "61ecace2-2a83-41eb-f2cf-8c72dc26416b"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    var video;\n",
              "    var div = null;\n",
              "    var stream;\n",
              "    var captureCanvas;\n",
              "    var imgElement;\n",
              "    var labelElement;\n",
              "\n",
              "    var pendingResolve = null;\n",
              "    var shutdown = false;\n",
              "\n",
              "    function removeDom() {\n",
              "       stream.getVideoTracks()[0].stop();\n",
              "       video.remove();\n",
              "       div.remove();\n",
              "       video = null;\n",
              "       div = null;\n",
              "       stream = null;\n",
              "       imgElement = null;\n",
              "       captureCanvas = null;\n",
              "       labelElement = null;\n",
              "    }\n",
              "\n",
              "    function onAnimationFrame() {\n",
              "      if (!shutdown) {\n",
              "        window.requestAnimationFrame(onAnimationFrame);\n",
              "      }\n",
              "      if (pendingResolve) {\n",
              "        var result = \"\";\n",
              "        if (!shutdown) {\n",
              "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
              "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
              "        }\n",
              "        var lp = pendingResolve;\n",
              "        pendingResolve = null;\n",
              "        lp(result);\n",
              "      }\n",
              "    }\n",
              "\n",
              "    async function createDom() {\n",
              "      if (div !== null) {\n",
              "        return stream;\n",
              "      }\n",
              "      div = document.createElement('div');\n",
              "      div.style.border = '2px solid black';\n",
              "      div.style.padding = '3px';\n",
              "      div.style.width = '100%';\n",
              "      div.style.maxWidth = '600px';\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const modelOut = document.createElement('div');\n",
              "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
              "      labelElement = document.createElement('span');\n",
              "      labelElement.innerText = 'No data';\n",
              "      labelElement.style.fontWeight = 'bold';\n",
              "      modelOut.appendChild(labelElement);\n",
              "      div.appendChild(modelOut);\n",
              "\n",
              "      video = document.createElement('video');\n",
              "      video.style.display = 'block';\n",
              "      video.width = div.clientWidth - 6;\n",
              "      video.setAttribute('playsinline', '');\n",
              "      video.onclick = () => { shutdown = true; };\n",
              "      stream = await navigator.mediaDevices.getUserMedia(\n",
              "          {video: { facingMode: \"user\"}});\n",
              "      div.appendChild(video);\n",
              "\n",
              "      imgElement = document.createElement('img');\n",
              "      imgElement.style.position = 'absolute';\n",
              "      imgElement.style.zIndex = 1;\n",
              "      imgElement.onclick = () => { shutdown = true; };\n",
              "      div.appendChild(imgElement);\n",
              "\n",
              "      const instruction = document.createElement('div');\n",
              "      instruction.innerHTML =\n",
              "          '<span style=\"color: red; font-weight: bold;\">' +\n",
              "          'When finished, click here or on the video to stop this demo</span>';\n",
              "      div.appendChild(instruction);\n",
              "      instruction.onclick = () => { shutdown = true; };\n",
              "\n",
              "      video.srcObject = stream;\n",
              "      await video.play();\n",
              "\n",
              "      captureCanvas = document.createElement('canvas');\n",
              "      captureCanvas.width = 640;\n",
              "      captureCanvas.height = 480;\n",
              "      window.requestAnimationFrame(onAnimationFrame);\n",
              "\n",
              "      return stream;\n",
              "    }\n",
              "    async function stream_frame(label, imgData) {\n",
              "      if (shutdown) {\n",
              "        removeDom();\n",
              "        shutdown = false;\n",
              "        return '';\n",
              "      }\n",
              "      var preCreate = Date.now();\n",
              "      stream = await createDom();\n",
              "\n",
              "      var preShow = Date.now();\n",
              "      if (label != \"\") {\n",
              "        labelElement.innerHTML = label;\n",
              "      }\n",
              "      if (imgData != \"\") {\n",
              "        var videoRect = video.getClientRects()[0];\n",
              "        imgElement.style.top = videoRect.top + \"px\";\n",
              "        imgElement.style.left = videoRect.left + \"px\";\n",
              "        imgElement.style.width = videoRect.width + \"px\";\n",
              "        imgElement.style.height = videoRect.height + \"px\";\n",
              "        imgElement.src = imgData;\n",
              "      }\n",
              "      var preCapture = Date.now();\n",
              "      var result = await new Promise(function(resolve, reject) {\n",
              "        pendingResolve = resolve;\n",
              "      });\n",
              "      shutdown = false;\n",
              "\n",
              "      return {'create': preShow - preCreate,\n",
              "              'show': preCapture - preShow,\n",
              "              'capture': Date.now() - preCapture,\n",
              "              'img': result};\n",
              "    }\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
            "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
            "/tmp/ipython-input-3496555551.py:10: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
            "  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[1;32m    643\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m         \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileno\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: '_idat' object has no attribute 'fileno'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1031319854.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;31m# 4. 출력용 이미지 생성 (RGBA)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0mimg_rgba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGBA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m     \u001b[0mbbox_bytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbbox_to_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_rgba\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m     \u001b[0mbbox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbbox_bytes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3496555551.py\u001b[0m in \u001b[0;36mbbox_to_bytes\u001b[0;34m(bbox_array)\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mbbox_PIL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPIL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbbox_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'RGBA'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0miobuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m   \u001b[0mbbox_PIL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miobuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m   \u001b[0mbbox_bytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'data:image/png;base64,{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb64encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miobuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mbbox_bytes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2587\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2588\u001b[0;31m             \u001b[0msave_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2589\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2590\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mopen_fp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/PIL/PngImagePlugin.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(im, fp, filename, chunk, save_all)\u001b[0m\n\u001b[1;32m   1493\u001b[0m         )\n\u001b[1;32m   1494\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msingle_im\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1495\u001b[0;31m         ImageFile._save(\n\u001b[0m\u001b[1;32m   1496\u001b[0m             \u001b[0msingle_im\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1497\u001b[0m             \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_idat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[1;32m    646\u001b[0m         \u001b[0m_encode_tile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbufsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedOperation\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m         \u001b[0m_encode_tile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbufsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"flush\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36m_encode_tile\u001b[0;34m(im, fp, tile, bufsize, fh, exc)\u001b[0m\n\u001b[1;32m    672\u001b[0m                     \u001b[0;31m# compress to Python file-compatible object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m                     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m                         \u001b[0merrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    675\u001b[0m                         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0merrcode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}