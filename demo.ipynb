{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYUIi6DbBOlH"
      },
      "source": [
        "## 0. 라이브러리 설치(세션 재시작 필요)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GJIFJySsMFi-",
        "outputId": "1efe9625-eb31-4c88-b962-61abcc64f47c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Collecting mediapipe\n",
            "  Downloading mediapipe-0.10.21-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (25.4.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (25.9.23)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.7.2)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.7.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from mediapipe) (3.10.0)\n",
            "INFO: pip is looking at multiple versions of mediapipe to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading mediapipe-0.10.20-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n",
            "  Downloading mediapipe-0.10.18-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\n",
            "  Downloading mediapipe-0.10.15-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\n",
            "  Downloading mediapipe-0.10.14-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.12/dist-packages (from mediapipe) (4.12.0.88)\n",
            "Collecting protobuf<5,>=4.25.3 (from mediapipe)\n",
            "  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Collecting sounddevice>=0.4.4 (from mediapipe)\n",
            "  Downloading sounddevice-0.5.3-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.7.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.76.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.18.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.11.12)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.12/dist-packages (from sounddevice>=0.4.4->mediapipe) (2.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.4)\n",
            "Requirement already satisfied: scipy>=1.13 in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe) (1.16.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (4.61.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.23)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading mediapipe-0.10.14-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.7/35.7 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sounddevice-0.5.3-py3-none-any.whl (32 kB)\n",
            "Installing collected packages: protobuf, sounddevice, mediapipe\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.5\n",
            "    Uninstalling protobuf-5.29.5:\n",
            "      Successfully uninstalled protobuf-5.29.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "opentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 4.25.8 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed mediapipe-0.10.14 protobuf-4.25.8 sounddevice-0.5.3\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "0dcdc84ef294412a9d575110727a2303",
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# 1. 라이브러리 설치 (최초 1회만 실행됨)\n",
        "!pip install opencv-python mediapipe tensorflow numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHrtIuOFXzkA"
      },
      "source": [
        "## 1. 라이브러리 로드 및 함수 선언(All_pose_classifier_keras.h5 필요)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_9n92iYYIeL"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "import joblib\n",
        "import tensorflow as tf\n",
        "import PIL.Image\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode, b64encode\n",
        "import os\n",
        "\n",
        "CURRENT_MODEL_TYPE = 'keras'\n",
        "\n",
        "def calculate_angle(a, b, c):\n",
        "    reliability = a[3] * b[3] * c[3]\n",
        "    if a[3] < 0.5 or b[3] < 0.5 or c[3] < 0.5: return None, reliability\n",
        "    a_xyz, b_xyz, c_xyz = a[:3], b[:3], c[:3]\n",
        "    try:\n",
        "        ba = a_xyz - b_xyz\n",
        "        bc = c_xyz - b_xyz\n",
        "        cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))\n",
        "        angle = np.degrees(np.arccos(np.clip(cosine_angle, -1.0, 1.0)))\n",
        "        return angle, reliability\n",
        "    except: return None, reliability\n",
        "\n",
        "def calculate_plane_normal(a, b, c):\n",
        "    reliability = a[3] * b[3] * c[3]\n",
        "    if a[3] < 0.5 or b[3] < 0.5 or c[3] < 0.5: return None, reliability\n",
        "    a_xyz, b_xyz, c_xyz = a[:3], b[:3], c[:3]\n",
        "    try:\n",
        "        v1 = b_xyz - a_xyz\n",
        "        v2 = c_xyz - a_xyz\n",
        "        normal = np.cross(v1, v2)\n",
        "        norm_len = np.linalg.norm(normal)\n",
        "        if norm_len == 0: return None, reliability\n",
        "        return normal / norm_len, reliability\n",
        "    except: return None, reliability\n",
        "\n",
        "def feature_engineering(landmarks):\n",
        "    \"\"\"\n",
        "    [Hybrid Feature Engineering]\n",
        "    1. Raw 좌표: 어깨 중심 + 어깨 너비 정규화 (Scale Invariant)\n",
        "    2. 기하학적 특징: 각도(0~1 정규화) + 법선 벡터\n",
        "    3. 신뢰도 정보 포함\n",
        "\n",
        "    모든 피처의 스케일을 대략 -1~1 또는 0~1로 맞춰서 결합합니다.\n",
        "    \"\"\"\n",
        "    if landmarks is None: return None\n",
        "\n",
        "    # ==========================================\n",
        "    # 1. Raw Coordinates Processing (어깨 중심 & 크기 보정)\n",
        "    # ==========================================\n",
        "    lm_copy = landmarks.copy()\n",
        "\n",
        "    # 인덱스\n",
        "    L_SHOULDER, R_SHOULDER = 11, 12\n",
        "\n",
        "    # 기준점(Origin): 어깨 중점\n",
        "    left_sh = lm_copy[L_SHOULDER, :3]\n",
        "    right_sh = lm_copy[R_SHOULDER, :3]\n",
        "    center_point = (left_sh + right_sh) / 2.0\n",
        "\n",
        "    # 크기 기준(Scale): 어깨 너비\n",
        "    shoulder_width = np.linalg.norm(left_sh - right_sh)\n",
        "    scale_factor = shoulder_width if shoulder_width > 1e-6 else 1.0\n",
        "\n",
        "    # (A) 위치 이동 및 스케일링 -> 결과범위: 대략 -1.0 ~ 1.0\n",
        "    lm_copy[:, :3] -= center_point\n",
        "    lm_copy[:, :3] /= scale_factor\n",
        "\n",
        "    # 1차원으로 펴기 (Visibility 포함 132차원)\n",
        "    raw_features = lm_copy.flatten()\n",
        "\n",
        "    # ==========================================\n",
        "    # 2. Geometric Features (각도 & 법선)\n",
        "    # ==========================================\n",
        "    # 인덱스 정의\n",
        "    NOSE = 0; L_EYE, R_EYE = 2, 5; L_EAR, R_EAR = 7, 8\n",
        "    L_ELBOW, R_ELBOW = 13, 14; L_WRIST, R_WRIST = 15, 16\n",
        "    L_HIP, R_HIP = 23, 24\n",
        "\n",
        "    angle_feats = []\n",
        "    normal_feats = []\n",
        "    reliability_feats = []\n",
        "\n",
        "    # (B) 각도 계산\n",
        "    angles_to_calc = [\n",
        "        (landmarks[L_SHOULDER], landmarks[L_ELBOW], landmarks[L_WRIST]),\n",
        "        (landmarks[R_SHOULDER], landmarks[R_ELBOW], landmarks[R_WRIST]),\n",
        "        (landmarks[L_ELBOW], landmarks[L_SHOULDER], landmarks[L_HIP]),\n",
        "        (landmarks[R_ELBOW], landmarks[R_SHOULDER], landmarks[R_HIP]),\n",
        "        (landmarks[L_EAR], landmarks[L_SHOULDER], landmarks[L_HIP]), # 거북목 확인용\n",
        "        (landmarks[R_EAR], landmarks[R_SHOULDER], landmarks[R_HIP])\n",
        "    ]\n",
        "\n",
        "    for p1, p2, p3 in angles_to_calc:\n",
        "        ang, rel = calculate_angle(p1, p2, p3)\n",
        "        # 각도 정규화: 0~180 -> 0.0~1.0\n",
        "        if ang is not None:\n",
        "            angle_feats.append(ang / 180.0)\n",
        "        else:\n",
        "            angle_feats.append(0.0)\n",
        "        reliability_feats.append(rel)\n",
        "\n",
        "    # (C) 법선 벡터 계산 (이미 -1~1 범위)\n",
        "    # Face Normal\n",
        "    face_norm, face_rel = calculate_plane_normal(landmarks[NOSE], landmarks[L_EYE], landmarks[R_EYE])\n",
        "    if face_norm is not None: normal_feats.extend(face_norm)\n",
        "    else: normal_feats.extend([0.0, 0.0, 0.0])\n",
        "    reliability_feats.append(face_rel)\n",
        "\n",
        "    # Chest/Shoulder Normal\n",
        "    chest_norm, chest_rel = calculate_plane_normal(landmarks[L_SHOULDER], landmarks[R_SHOULDER], landmarks[NOSE])\n",
        "    if chest_norm is not None: normal_feats.extend(chest_norm)\n",
        "    else: normal_feats.extend([0.0, 0.0, 0.0])\n",
        "    reliability_feats.append(chest_rel)\n",
        "\n",
        "    # ==========================================\n",
        "    # 3. Feature Concatenation (결합)\n",
        "    # ==========================================\n",
        "    # 리스트들을 numpy array로 변환\n",
        "    geo_vector = np.array(angle_feats + normal_feats + reliability_feats)\n",
        "\n",
        "    # [최종 결합] Raw 좌표 벡터 + 기하학적 특징 벡터\n",
        "    # 차원 수: 132 (Raw) + 6 (Angles) + 6 (Normals) + 8 (Reliability) = 152차원\n",
        "    final_features = np.concatenate([raw_features, geo_vector])\n",
        "\n",
        "    return final_features\n",
        "\n",
        "MODEL_PATHS = {\n",
        "    'rf': 'All_pose_classifier_rf.pkl',\n",
        "    'svm': 'All_pose_classifier_svm.pkl',\n",
        "    'keras': 'All_pose_classifier_keras.h5'\n",
        "}\n",
        "\n",
        "def load_pose_model(model_type, paths):\n",
        "    path = paths.get(model_type)\n",
        "\n",
        "    if not os.path.exists(path):\n",
        "        print(f\"오류: '{path}' 파일이 없습니다. 경로를 확인하세요.\")\n",
        "        return None\n",
        "\n",
        "    if model_type in ['rf', 'svm']:\n",
        "        # Scikit-learn 계열 (joblib 사용)\n",
        "        model = joblib.load(path)\n",
        "        print(f\"[{model_type.upper()}] Scikit-learn 모델 로드 완료!\")\n",
        "        return model\n",
        "\n",
        "    elif model_type == 'keras':\n",
        "        # Deep Learning 계열 (Keras 사용)\n",
        "        model = tf.keras.models.load_model(path)\n",
        "        print(f\"[{model_type.upper()}] Keras 모델 로드 완료!\")\n",
        "        return model\n",
        "\n",
        "    else:\n",
        "        print(\"지원하지 않는 모델 타입입니다.\")\n",
        "        return None\n",
        "\n",
        "# 모델 로드 실행\n",
        "model = load_pose_model(CURRENT_MODEL_TYPE, MODEL_PATHS)\n",
        "\n",
        "# ==========================================\n",
        "#  예측 수행 함수\n",
        "# ==========================================\n",
        "def predict_pose(model, features, model_type):\n",
        "    \"\"\"\n",
        "    모델 타입에 따라 알맞은 전처리와 예측 결과를 반환합니다.\n",
        "    \"\"\"\n",
        "    if model is None or features is None:\n",
        "        return None\n",
        "\n",
        "    # 1) Scikit-learn 계열 (RF, SVM)\n",
        "    if model_type in ['rf', 'svm']:\n",
        "        # 입력: 2차원 배열 필요 (1, N) -> reshape(1, -1) 권장\n",
        "        input_data = features.reshape(1, -1)\n",
        "        prediction = model.predict(input_data)[0] # 결과가 배열로 나오므로 첫 번째 요소 선택\n",
        "        return prediction\n",
        "\n",
        "    # 2) Keras 계열 (Deep Learning)\n",
        "    elif model_type == 'keras':\n",
        "        # 입력: 반드시 (1, features_len) 형태여야 함\n",
        "        input_data = features.reshape(1, -1)\n",
        "\n",
        "        # 예측: 확률 분포 반환 (예: [0.1, 0.8, 0.1])\n",
        "        prediction_probs = model.predict(input_data, verbose=0)\n",
        "\n",
        "        # 가장 높은 확률의 인덱스 추출\n",
        "        prediction = np.argmax(prediction_probs)\n",
        "        return prediction\n",
        "\n",
        "# ==========================================\n",
        "# MediaPipe 설정\n",
        "# ==========================================\n",
        "mp_pose = mp.solutions.pose\n",
        "pose = mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "\n",
        "print(f\"\\n현재 설정된 모델: {CURRENT_MODEL_TYPE}\")\n",
        "\n",
        "def js_to_image(js_reply):\n",
        "  \"\"\"JS 객체를 OpenCV 이미지로 변환\"\"\"\n",
        "  image_bytes = b64decode(js_reply.split(',')[1])\n",
        "  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "  img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "  return img\n",
        "\n",
        "def bbox_to_bytes(bbox_array):\n",
        "  \"\"\"OpenCV 이미지를 JS 전송용 byte로 변환\"\"\"\n",
        "  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n",
        "  iobuf = io.BytesIO()\n",
        "  bbox_PIL.save(iobuf, format='png')\n",
        "  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "  return bbox_bytes\n",
        "\n",
        "def video_stream():\n",
        "  js = Javascript('''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "    var captureCanvas;\n",
        "    var imgElement;\n",
        "    var labelElement;\n",
        "\n",
        "    var pendingResolve = null;\n",
        "    var shutdown = false;\n",
        "\n",
        "    function removeDom() {\n",
        "       stream.getVideoTracks()[0].stop();\n",
        "       video.remove();\n",
        "       div.remove();\n",
        "       video = null;\n",
        "       div = null;\n",
        "       stream = null;\n",
        "       imgElement = null;\n",
        "       captureCanvas = null;\n",
        "       labelElement = null;\n",
        "    }\n",
        "\n",
        "    function onAnimationFrame() {\n",
        "      if (!shutdown) {\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "      }\n",
        "      if (pendingResolve) {\n",
        "        var result = \"\";\n",
        "        if (!shutdown) {\n",
        "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
        "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
        "        }\n",
        "        var lp = pendingResolve;\n",
        "        pendingResolve = null;\n",
        "        lp(result);\n",
        "      }\n",
        "    }\n",
        "\n",
        "    async function createDom() {\n",
        "      if (div !== null) {\n",
        "        return stream;\n",
        "      }\n",
        "      div = document.createElement('div');\n",
        "      div.style.border = '2px solid black';\n",
        "      div.style.padding = '3px';\n",
        "      div.style.width = '100%';\n",
        "      div.style.maxWidth = '600px';\n",
        "      document.body.appendChild(div);\n",
        "\n",
        "      const modelOut = document.createElement('div');\n",
        "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
        "      labelElement = document.createElement('span');\n",
        "      labelElement.innerText = 'No data';\n",
        "      labelElement.style.fontWeight = 'bold';\n",
        "      modelOut.appendChild(labelElement);\n",
        "      div.appendChild(modelOut);\n",
        "\n",
        "      video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      video.width = div.clientWidth - 6;\n",
        "      video.setAttribute('playsinline', '');\n",
        "      video.onclick = () => { shutdown = true; };\n",
        "      stream = await navigator.mediaDevices.getUserMedia(\n",
        "          {video: { facingMode: \"user\"}});\n",
        "      div.appendChild(video);\n",
        "\n",
        "      imgElement = document.createElement('img');\n",
        "      imgElement.style.position = 'absolute';\n",
        "      imgElement.style.zIndex = 1;\n",
        "      imgElement.onclick = () => { shutdown = true; };\n",
        "      div.appendChild(imgElement);\n",
        "\n",
        "      const instruction = document.createElement('div');\n",
        "      instruction.innerHTML =\n",
        "          '<span style=\"color: red; font-weight: bold;\">' +\n",
        "          'When finished, click here or on the video to stop this demo</span>';\n",
        "      div.appendChild(instruction);\n",
        "      instruction.onclick = () => { shutdown = true; };\n",
        "\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      captureCanvas = document.createElement('canvas');\n",
        "      captureCanvas.width = 640;\n",
        "      captureCanvas.height = 480;\n",
        "      window.requestAnimationFrame(onAnimationFrame);\n",
        "\n",
        "      return stream;\n",
        "    }\n",
        "    async function stream_frame(label, imgData) {\n",
        "      if (shutdown) {\n",
        "        removeDom();\n",
        "        shutdown = false;\n",
        "        return '';\n",
        "      }\n",
        "      var preCreate = Date.now();\n",
        "      stream = await createDom();\n",
        "\n",
        "      var preShow = Date.now();\n",
        "      if (label != \"\") {\n",
        "        labelElement.innerHTML = label;\n",
        "      }\n",
        "      if (imgData != \"\") {\n",
        "        var videoRect = video.getClientRects()[0];\n",
        "        imgElement.style.top = videoRect.top + \"px\";\n",
        "        imgElement.style.left = videoRect.left + \"px\";\n",
        "        imgElement.style.width = videoRect.width + \"px\";\n",
        "        imgElement.style.height = videoRect.height + \"px\";\n",
        "        imgElement.src = imgData;\n",
        "      }\n",
        "      var preCapture = Date.now();\n",
        "      var result = await new Promise(function(resolve, reject) {\n",
        "        pendingResolve = resolve;\n",
        "      });\n",
        "      shutdown = false;\n",
        "\n",
        "      return {'create': preShow - preCreate,\n",
        "              'show': preCapture - preShow,\n",
        "              'capture': Date.now() - preCapture,\n",
        "              'img': result};\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "\n",
        "def video_frame(label, bbox):\n",
        "  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n",
        "  return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccKVoOy-BhT0"
      },
      "source": [
        "# 2. 메인 실행"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "on6u6v32A94q",
        "outputId": "61ecace2-2a83-41eb-f2cf-8c72dc26416b"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    var video;\n    var div = null;\n    var stream;\n    var captureCanvas;\n    var imgElement;\n    var labelElement;\n\n    var pendingResolve = null;\n    var shutdown = false;\n\n    function removeDom() {\n       stream.getVideoTracks()[0].stop();\n       video.remove();\n       div.remove();\n       video = null;\n       div = null;\n       stream = null;\n       imgElement = null;\n       captureCanvas = null;\n       labelElement = null;\n    }\n\n    function onAnimationFrame() {\n      if (!shutdown) {\n        window.requestAnimationFrame(onAnimationFrame);\n      }\n      if (pendingResolve) {\n        var result = \"\";\n        if (!shutdown) {\n          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n        }\n        var lp = pendingResolve;\n        pendingResolve = null;\n        lp(result);\n      }\n    }\n\n    async function createDom() {\n      if (div !== null) {\n        return stream;\n      }\n      div = document.createElement('div');\n      div.style.border = '2px solid black';\n      div.style.padding = '3px';\n      div.style.width = '100%';\n      div.style.maxWidth = '600px';\n      document.body.appendChild(div);\n\n      const modelOut = document.createElement('div');\n      modelOut.innerHTML = \"<span>Status:</span>\";\n      labelElement = document.createElement('span');\n      labelElement.innerText = 'No data';\n      labelElement.style.fontWeight = 'bold';\n      modelOut.appendChild(labelElement);\n      div.appendChild(modelOut);\n\n      video = document.createElement('video');\n      video.style.display = 'block';\n      video.width = div.clientWidth - 6;\n      video.setAttribute('playsinline', '');\n      video.onclick = () => { shutdown = true; };\n      stream = await navigator.mediaDevices.getUserMedia(\n          {video: { facingMode: \"user\"}});\n      div.appendChild(video);\n\n      imgElement = document.createElement('img');\n      imgElement.style.position = 'absolute';\n      imgElement.style.zIndex = 1;\n      imgElement.onclick = () => { shutdown = true; };\n      div.appendChild(imgElement);\n\n      const instruction = document.createElement('div');\n      instruction.innerHTML =\n          '<span style=\"color: red; font-weight: bold;\">' +\n          'When finished, click here or on the video to stop this demo</span>';\n      div.appendChild(instruction);\n      instruction.onclick = () => { shutdown = true; };\n\n      video.srcObject = stream;\n      await video.play();\n\n      captureCanvas = document.createElement('canvas');\n      captureCanvas.width = 640;\n      captureCanvas.height = 480;\n      window.requestAnimationFrame(onAnimationFrame);\n\n      return stream;\n    }\n    async function stream_frame(label, imgData) {\n      if (shutdown) {\n        removeDom();\n        shutdown = false;\n        return '';\n      }\n      var preCreate = Date.now();\n      stream = await createDom();\n\n      var preShow = Date.now();\n      if (label != \"\") {\n        labelElement.innerHTML = label;\n      }\n      if (imgData != \"\") {\n        var videoRect = video.getClientRects()[0];\n        imgElement.style.top = videoRect.top + \"px\";\n        imgElement.style.left = videoRect.left + \"px\";\n        imgElement.style.width = videoRect.width + \"px\";\n        imgElement.style.height = videoRect.height + \"px\";\n        imgElement.src = imgData;\n      }\n      var preCapture = Date.now();\n      var result = await new Promise(function(resolve, reject) {\n        pendingResolve = resolve;\n      });\n      shutdown = false;\n\n      return {'create': preShow - preCreate,\n              'show': preCapture - preShow,\n              'capture': Date.now() - preCapture,\n              'img': result};\n    }\n    ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
            "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
            "/tmp/ipython-input-3496555551.py:10: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
            "  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[1;32m    643\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m         \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileno\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: '_idat' object has no attribute 'fileno'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1031319854.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;31m# 4. 출력용 이미지 생성 (RGBA)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0mimg_rgba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGBA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m     \u001b[0mbbox_bytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbbox_to_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_rgba\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m     \u001b[0mbbox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbbox_bytes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3496555551.py\u001b[0m in \u001b[0;36mbbox_to_bytes\u001b[0;34m(bbox_array)\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mbbox_PIL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPIL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbbox_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'RGBA'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0miobuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m   \u001b[0mbbox_PIL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miobuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m   \u001b[0mbbox_bytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'data:image/png;base64,{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb64encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miobuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mbbox_bytes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2587\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2588\u001b[0;31m             \u001b[0msave_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2589\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2590\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mopen_fp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/PIL/PngImagePlugin.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(im, fp, filename, chunk, save_all)\u001b[0m\n\u001b[1;32m   1493\u001b[0m         )\n\u001b[1;32m   1494\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msingle_im\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1495\u001b[0;31m         ImageFile._save(\n\u001b[0m\u001b[1;32m   1496\u001b[0m             \u001b[0msingle_im\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1497\u001b[0m             \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_idat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[1;32m    646\u001b[0m         \u001b[0m_encode_tile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbufsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedOperation\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m         \u001b[0m_encode_tile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbufsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"flush\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36m_encode_tile\u001b[0;34m(im, fp, tile, bufsize, fh, exc)\u001b[0m\n\u001b[1;32m    672\u001b[0m                     \u001b[0;31m# compress to Python file-compatible object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m                     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m                         \u001b[0merrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    675\u001b[0m                         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0merrcode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "def predict_wrapper(model, features, model_type):\n",
        "    \"\"\"\n",
        "    모델 타입에 따라 적절한 예측 함수를 호출하고,\n",
        "    (상태 문자열, 'Good'일 확률)을 반환합니다.\n",
        "    \"\"\"\n",
        "    if features is None:\n",
        "        return \"No Pose\", 0.0\n",
        "\n",
        "    # 입력 차원 맞추기 (1, N)\n",
        "    input_data = features.reshape(1, -1)\n",
        "\n",
        "    prob_good = 0.0\n",
        "\n",
        "    try:\n",
        "        # A. 머신러닝 모델 (RF, SVM)\n",
        "        if model_type in ['rf', 'svm']:\n",
        "            # predict_proba 반환값: [[prob_bad, prob_good]]\n",
        "            probs = model.predict_proba(input_data)[0]\n",
        "            prob_good = probs[1] # 1번 인덱스가 Good이라고 가정\n",
        "\n",
        "        # B. 딥러닝 모델 (Keras)\n",
        "        elif model_type == 'keras':\n",
        "            # 반환값 형태: [[0.78]] (2차원 배열 안에 값 1개)\n",
        "            pred = model.predict(input_data, verbose=0)\n",
        "\n",
        "            prob_good = pred[0][0]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Prediction Logic Error: {e}\")\n",
        "        return \"Error\", 0.0\n",
        "\n",
        "    # 상태 결정 (임계값 0.5)\n",
        "    if prob_good > 0.5:\n",
        "        return \"Good\", prob_good\n",
        "    else:\n",
        "        return \"Bad\", prob_good\n",
        "\n",
        "# 스트리밍 시작 (Colab 환경 가정)\n",
        "try:\n",
        "    video_stream() \n",
        "except NameError:\n",
        "    print(\"video_stream() 함수가 정의되지 않았습니다. 이전 셀을 실행했는지 확인하세요.\")\n",
        "\n",
        "label_html = 'Starting Posture Analysis...'\n",
        "bbox = ''\n",
        "count = 0\n",
        "\n",
        "while True:\n",
        "    # Colab JS 프록시를 통해 프레임 받기\n",
        "    try:\n",
        "        js_reply = video_frame(label_html, bbox)\n",
        "        if not js_reply: break\n",
        "    except: break\n",
        "\n",
        "    # 1. 이미지 변환 및 전처리\n",
        "    img = js_to_image(js_reply[\"img\"])\n",
        "    img = cv2.flip(img, 1) # 거울 모드\n",
        "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # 2. MediaPipe 분석\n",
        "    results = pose.process(img_rgb)\n",
        "\n",
        "    status = \"No Pose\"\n",
        "    prediction_prob = 0.0\n",
        "    color = (200, 200, 200) # 회색 (Default)\n",
        "\n",
        "    # 3. 랜드마크 감지 시 처리\n",
        "    if results.pose_landmarks:\n",
        "        try:\n",
        "            # (1) 랜드마크 추출 및 Numpy 변환\n",
        "            landmarks_list = []\n",
        "            for lm in results.pose_landmarks.landmark:\n",
        "                landmarks_list.append([lm.x, lm.y, lm.z, lm.visibility])\n",
        "            landmarks_arr = np.array(landmarks_list)\n",
        "\n",
        "            # (2) Feature Engineering (기존 함수 사용)\n",
        "            features = feature_engineering(landmarks_arr)\n",
        "\n",
        "            if features is not None:\n",
        "                # (3) [모듈화된 예측 함수 호출]\n",
        "                # 여기서 모델 타입에 따라 알아서 처리됩니다.\n",
        "                status, prediction_prob = predict_wrapper(model, features, CURRENT_MODEL_TYPE)\n",
        "\n",
        "                # 색상 설정\n",
        "                if status == \"Good\":\n",
        "                    color = (0, 255, 0) # 초록\n",
        "                elif status == \"Bad\":\n",
        "                    color = (0, 0, 255) # 빨강\n",
        "                else:\n",
        "                    color = (0, 255, 255) # 노랑 (에러 등)\n",
        "\n",
        "                label_html = f\"[{CURRENT_MODEL_TYPE.upper()}] Status: {status} ({prediction_prob*100:.1f}%)\"\n",
        "\n",
        "            # (4) 시각화: 랜드마크 그리기\n",
        "            mp_drawing.draw_landmarks(\n",
        "                img,\n",
        "                results.pose_landmarks,\n",
        "                mp_pose.POSE_CONNECTIONS,\n",
        "                mp_drawing.DrawingSpec(color=color, thickness=2, circle_radius=2),\n",
        "                mp_drawing.DrawingSpec(color=(255,255,255), thickness=2, circle_radius=2)\n",
        "            )\n",
        "\n",
        "            # (5) 시각화: 텍스트 오버레이\n",
        "            cv2.rectangle(img, (0,0), (300, 60), (245, 117, 16), -1)\n",
        "            cv2.putText(img, f\"{status} {prediction_prob*100:.0f}%\", (10, 40),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
        "            cv2.putText(img, f\"Model: {CURRENT_MODEL_TYPE}\", (10, 15), # 현재 모델 표시\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 200), 1)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Main Loop Error: {e}\")\n",
        "            pass\n",
        "    else:\n",
        "        label_html = 'Pose Not Detected'\n",
        "\n",
        "    # 4. 출력용 이미지 생성 (RGBA)\n",
        "    img_rgba = cv2.cvtColor(img, cv2.COLOR_BGR2RGBA)\n",
        "    bbox_bytes = bbox_to_bytes(img_rgba)\n",
        "    bbox = bbox_bytes"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "gblXfDYBbsye",
        "C9um-Rfkbx-D",
        "UcWi3mJ-V5ZN",
        "H_tJKxjaBZz5"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
